{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from projection_model.models import make_model_vocab\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "from os.path import join\n",
    "from syntax_trees.syntax_tree import SyntaxTree\n",
    "from thesaurus_parsing.thesaurus_parser import ThesaurusParser\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построение модели логистической регрессии\n",
    "\n",
    "Для каждой пары запросов требуется построить вектор фичей - количество раз, которое встречалась эта пара слов в заданном паттерне. Такие фичи возьмём для топа популярных $500$ синтаксических паттернов.\n",
    "\n",
    "Проблема в том, что пар слов очень много. Но надо посмотреть, сколько вообще раз встречаются в текстах первые 500 паттернов.\n",
    "\n",
    "Загрузим самые популярные паттерны, положим их в сет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/popular_patterns.csv') as patterns_file:\n",
    "    popular_patterns = patterns_file.readlines()\n",
    "popular_patterns = [pattern[:-1] if pattern[-1] == '\\n' else pattern for pattern in popular_patterns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_patterns_set = set(popular_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesaurus = ThesaurusParser(\"../data/RuThes\", need_closure=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим также словарь с самыми популярными словами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-bf25c58ab444>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_model_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/MIPT/HypernymyDetection/projection_model/models.py\u001b[0m in \u001b[0;36mmake_model_vocab\u001b[0;34m(embedder, thesaurus, cut_most_common)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthesaurus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_entries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mlemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemma'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mvocab_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sentence_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mDIR_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/loginov-ra/MIPT/HypernymyDetection/data/Lenta/texts_tagged_processed_tree\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fasttext/FastText.py\u001b[0m in \u001b[0;36mget_sentence_vector\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetSentenceVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vocab = make_model_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_keys = list(vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_keys_set = set(vocab_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заведём просто счётчик встречаемости троек (гипоним, гипероним, паттерн) и посмотрим, вместится ли это в память"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH = \"/home/loginov-ra/MIPT/HypernymyDetection/data/Lenta/texts_tagged_processed_tree\"\n",
    "file_list = os.listdir(DIR_PATH)\n",
    "file_list = [join(DIR_PATH, filename) for filename in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_hyponym_hypernym(hypo_cand, hyper_cand):\n",
    "    if hypo_cand not in thesaurus.hypernyms_dict:\n",
    "        return False\n",
    "    return hyper_cand in thesaurus.hypernyms_dict[hypo_cand]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hypernymy_pairs(multitokens):\n",
    "    pairs = []\n",
    "    for i, hypernym_candidate in enumerate(multitokens):\n",
    "        for j, hyponym_candidate in enumerate(multitokens):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if is_hyponym_hypernym(hyponym_candidate, hypernym_candidate):\n",
    "                pairs.append((j, i))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24bc984cc034a349935d9cec8a1a5f9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "feature_counter = Counter()\n",
    "\n",
    "for filename in tqdm(file_list):\n",
    "    with open(filename, encoding='utf-8') as sentences_file:\n",
    "        sentences = json.load(sentences_file)\n",
    "        for sent in sentences:\n",
    "            if 'deeppavlov' not in sent:\n",
    "                continue\n",
    "            \n",
    "            multitokens, main_pos = sent['multi']\n",
    "            lemmas = sent['deeppavlov']\n",
    "            pos = sent['pos']\n",
    "            tree_info = sent['syntax']\n",
    "            \n",
    "            if len(multitokens) > 100:\n",
    "                continue\n",
    "            \n",
    "            tree = SyntaxTree(empty=True)\n",
    "            tree.load_from_json(tree_info)\n",
    "            \n",
    "            in_vocab = [token in vocab_keys_set for token in multitokens]\n",
    "            \n",
    "            for hypo_multi in range(len(multitokens)):\n",
    "                for hyper_multi in range(len(multitokens)):\n",
    "                    if hypo_multi == hyper_multi:\n",
    "                        continue\n",
    "                    \n",
    "                    if not in_vocab[hyper_multi] or not in_vocab[hypo_multi]:\n",
    "                        continue\n",
    "                    \n",
    "                    hypo_main, hyper_main = main_pos[hypo_multi], main_pos[hyper_multi]\n",
    "                    try:\n",
    "                        pattern = tree.get_syntax_pattern(hypo_main, hyper_main, pos, lemmas)\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "                    if pattern is None:\n",
    "                        continue\n",
    "                    pattern = ';'.join(pattern)\n",
    "                    hyponym = multitokens[hypo_multi]\n",
    "                    hypernym = multitokens[hyper_multi]\n",
    "                    if pattern in popular_patterns_set:\n",
    "                        feature_counter[(hyponym, hypernym, pattern)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('сообщать', 'ссылка', '{}:VERB:obl:NOUN:{}'), 3960),\n",
       " (('ссылка', 'сообщать', '{}:NOUN:obl:VERB:{}'), 3960),\n",
       " (('риа', 'новость', '{}:X:appos:NOUN:{}'), 3280),\n",
       " (('сообщаться', 'сайт', '{}:VERB:obl:NOUN:{}'), 1987),\n",
       " (('сайт', 'сообщаться', '{}:NOUN:obl:VERB:{}'), 1987),\n",
       " (('миллион', 'доллар', '{}:NOUN:nmod:NOUN:{}'), 1779),\n",
       " (('доллар', 'миллион', '{}:NOUN:nmod:NOUN:{}'), 1779),\n",
       " (('сообщать', 'интерфакс', '{}:VERB:nsubj:NOUN:{}'), 1777),\n",
       " (('интерфакс', 'сообщать', '{}:NOUN:nsubj:VERB:{}'), 1777),\n",
       " (('премьер', 'министр', '{}:NOUN:appos:NOUN:{}'), 1683)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5efdca4a6a4fd78aeff88b5ab2e808"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pair_pattern_features = dict()\n",
    "\n",
    "for key, cnt in tqdm(feature_counter.items()):\n",
    "    hyponym, hypernym, pattern = key\n",
    "    \n",
    "    if hyponym not in pair_pattern_features:\n",
    "        pair_pattern_features[hyponym] = dict()\n",
    "    \n",
    "    if hypernym not in pair_pattern_features[hyponym]:\n",
    "        pair_pattern_features[hyponym][hypernym] = Counter()\n",
    "        \n",
    "    pair_pattern_features[hyponym][hypernym][pattern] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Файл с такими встречаемостями был записан, восстановим его"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('freq_features1000.json') as features_file:\n",
    "    pair_pattern_features = json.load(features_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем сразу вектора встречаемости паттернов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf2a2428cc4947b686650ace23ff31ce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vectorized_pattern_features = dict()\n",
    "\n",
    "for hyponym in tqdm(pair_pattern_features.keys()):\n",
    "    vectorized_pattern_features[hyponym] = dict()\n",
    "    for hypernym, pattern_ctr in pair_pattern_features[hyponym].items():\n",
    "        cnt_list = []\n",
    "        for pattern in popular_patterns:\n",
    "            if pattern in pattern_ctr:\n",
    "                cnt_list.append(pattern_ctr[pattern])\n",
    "            else:\n",
    "                cnt_list.append(0)\n",
    "        vectorized_pattern_features[hyponym][hypernym] = cnt_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________\n",
    "\n",
    "Теперь требуется собственно обучить логистическую регрессию. Для этого надо собрать обучающую выборку и засунуть в `sklearn`. Обучение составим так же с помощью случайного майнинга негативных примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e32a36f73d48f18cb852761910a233"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_negative = 2\n",
    "\n",
    "for hyponym, hypernyms in tqdm(thesaurus.hypernyms_dict.items()):\n",
    "    if hyponym not in vectorized_pattern_features:\n",
    "        continue\n",
    "    for hypernym in hypernyms:\n",
    "        if hypernym not in vectorized_pattern_features[hyponym]:\n",
    "            continue\n",
    "        X.append(vectorized_pattern_features[hyponym][hypernym])\n",
    "        y.append(1)\n",
    "        \n",
    "        for i in range(n_negative):\n",
    "            all_pairs = list(vectorized_pattern_features[hyponym].keys())\n",
    "            neg_hypernym = np.random.choice(all_pairs)\n",
    "            X.append(vectorized_pattern_features[hyponym][neg_hypernym])\n",
    "            y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vectorized_pattern_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25161"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что примеров не очень много, поскольку в тезаурусе всё же есть довольно редкие пары гипоним-гипероним, и в тексте они не встречались. Поскольку на такие пары смотреть и не хочется, оставим то, что есть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20128, 5033)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(solver='lbfgs', max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': np.logspace(-4, 2, 10),\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['saga'],\n",
    "    'max_iter': [300]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(lr_model, param_grid, scoring='accuracy', verbose=10, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loginov-ra/.local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:  7.9min\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=4)]: Done  53 tasks      | elapsed: 14.5min\n",
      "[Parallel(n_jobs=4)]: Done  60 out of  60 | elapsed: 16.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  60 out of  60 | elapsed: 16.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=2000, multi_class='warn',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=None, solver='lbfgs',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='warn', n_jobs=4,\n",
       "             param_grid={'C': array([1.00000000e-04, 4.64158883e-04, 2.15443469e-03, 1.00000000e-02,\n",
       "       4.64158883e-02, 2.15443469e-01, 1.00000000e+00, 4.64158883e+00,\n",
       "       2.15443469e+01, 1.00000000e+02]),\n",
       "                         'max_iter': [300], 'penalty': ['l1', 'l2'],\n",
       "                         'solver': ['saga']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0, 'max_iter': 300, 'penalty': 'l2', 'solver': 'saga'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7383270415259289"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что качество довольно хорошее при наличии таких пар в текстах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=2000,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.max_iter = 2000\n",
    "best_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним эту модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(best_model, open('../data/lr500.bin', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.85722543,  0.99021595, -0.66331773,  0.84732681,  1.24377586,\n",
       "        0.84059946,  0.        ,  0.        ,  0.54562648,  0.28759792])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.coef_[0][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
