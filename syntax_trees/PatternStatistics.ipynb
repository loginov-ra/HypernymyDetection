{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from thesaurus_parsing.thesaurus_parser import ThesaurusParser\n",
    "from syntax_tree import SyntaxTree\n",
    "from os.path import join\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение паттернов гиперонимии\n",
    "\n",
    "Воспользуемся тем, что теперь все данные лежат в удобном формате. Построим тезаурус и будем искать паттерны гиперонимии.\n",
    "\n",
    "Сначала прости считаем имеющийся список файлов с обработанными текстами, которые имеются на данный момент (пока что идёт обработка 36к документов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH = \"/home/loginov-ra/MIPT/HypernymyDetection/data/Lenta/texts_tagged_processed_tree\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir(DIR_PATH)\n",
    "file_list = [join(DIR_PATH, filename) for filename in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36437"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/loginov-ra/MIPT/HypernymyDetection/data/Lenta/texts_tagged_processed_tree/20100603protes.txt_processed.json'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В каждом файле известен набор синтаксических деревьев и лемматизированных предложений. Загрузим словарь гиперонимов и посмотрим, как проверять в нём наличие отношения гиперонимии между двумя заданными сущностями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesaurus = ThesaurusParser(\"../data/RuThes\", need_closure=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['живой организм',\n",
       " 'макроорганизм',\n",
       " 'живое',\n",
       " 'существо',\n",
       " 'живой существо',\n",
       " 'биологический',\n",
       " 'организм',\n",
       " 'живность',\n",
       " 'индивидуум',\n",
       " 'биологический организм',\n",
       " 'субъект деятельность',\n",
       " 'особь']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thesaurus.hypernyms_dict['человек']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем проходить по каждому предложению, искать сущности, которые являются гиперонимами и строить синтаксический паттерн из записанного в данных дерева. По каждому паттерну заведём статистику встречаемости.\n",
    "\n",
    "**Важно.** Эту процедуру надо производить только на обучающей части данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_hyponym_hypernym(hypo_cand, hyper_cand):\n",
    "    if hypo_cand not in thesaurus.hypernyms_dict:\n",
    "        return False\n",
    "    return hyper_cand in thesaurus.hypernyms_dict[hypo_cand]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hypernymy_pairs(multitokens):\n",
    "    pairs = []\n",
    "    for i, hypernym_candidate in enumerate(multitokens):\n",
    "        for j, hyponym_candidate in enumerate(multitokens):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if is_hyponym_hypernym(hyponym_candidate, hypernym_candidate):\n",
    "                pairs.append((j, i))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90bd8e2904824ef499852f79433e6b58"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pattern_counter = Counter()\n",
    "\n",
    "no_deeppavlov = 0\n",
    "tree_failures = 0\n",
    "\n",
    "for filename in tqdm(file_list):\n",
    "    with open(filename, encoding='utf-8') as sentences_file:\n",
    "        sentences = json.load(sentences_file)\n",
    "        for sent in sentences:\n",
    "            if 'deeppavlov' not in sent:\n",
    "                no_deeppavlov += 1\n",
    "                continue\n",
    "            \n",
    "            multitokens, main_pos = sent['multi']\n",
    "            lemmas = sent['deeppavlov']\n",
    "            pos = sent['pos']\n",
    "            tree_info = sent['syntax']\n",
    "            \n",
    "            tree = SyntaxTree(empty=True)\n",
    "            tree.load_from_json(tree_info)\n",
    "            \n",
    "            for pattern_pair in get_hypernymy_pairs(multitokens):\n",
    "                hypo_multi, hyper_multi = pattern_pair\n",
    "                hypo_main, hyper_main = main_pos[hypo_multi], main_pos[hyper_multi]\n",
    "                #try:\n",
    "                pattern = tree.get_syntax_pattern(hypo_main, hyper_main, pos, lemmas)\n",
    "                #except:\n",
    "                #    tree_failures += 1\n",
    "                #    continue\n",
    "                    \n",
    "                if pattern is None:\n",
    "                    tree_failures += 1\n",
    "                    continue\n",
    "                pattern = ';'.join(pattern)\n",
    "                pattern_counter[pattern] += 1\n",
    "                \n",
    "                #if pattern == '{}:NOUN:conj:NOUN:{}':\n",
    "                #    print(' '.join(sent['initial']))\n",
    "                #    print(multitokens[hypo_multi], multitokens[hyper_multi])\n",
    "                #    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('{}:NOUN:nmod:NOUN:{}', 4641),\n",
       " ('{}:NOUN:appos:NOUN:{}', 1805),\n",
       " ('{}:NOUN:conj:NOUN:{};{}:NOUN:cc:CCONJ:и', 1179),\n",
       " ('{}:VERB:parataxis:VERB:{}', 1079),\n",
       " ('{}:NOUN:conj:NOUN:{}', 1073),\n",
       " ('{}:ADJ:amod:NOUN:{}', 1015),\n",
       " ('{}:NOUN:obl:VERB:{}', 1000),\n",
       " ('{}:VERB:conj:VERB:{}', 987),\n",
       " ('и:CCONJ:cc:NOUN:{};{}:NOUN:conj:NOUN:{}', 985),\n",
       " ('{}:VERB:advcl:VERB:{}', 842),\n",
       " ('{}:VERB:aux:AUX:{}', 772),\n",
       " ('{}:VERB:parataxis:VERB:{};{}:VERB:mark:SCONJ:как', 679),\n",
       " ('{}:VERB:obl:NOUN:{}', 595),\n",
       " ('{}:NOUN:parataxis:NOUN:{}', 578),\n",
       " ('{}:VERB:aux:pass:AUX:{}', 567),\n",
       " ('{}:VERB:conj:VERB:{};{}:VERB:cc:CCONJ:и', 505),\n",
       " ('{}:VERB:ccomp:VERB:{}', 498),\n",
       " ('и:CCONJ:cc:VERB:{};{}:VERB:conj:VERB:{}', 488),\n",
       " ('{}:VERB:xcomp:VERB:{}', 465),\n",
       " ('{}:VERB:obj:NOUN:{}', 383),\n",
       " ('{}:NOUN:nsubj:VERB:{}', 315),\n",
       " ('{}:NOUN:obj:VERB:{}', 303),\n",
       " ('{}:VERB:xcomp:ADJ:должный;должный:ADJ:cop:AUX:{}', 296),\n",
       " ('{}:VERB:conj:VERB:{};{}:VERB:cc:CCONJ:а', 256),\n",
       " ('{}:NOUN:nsubj:NOUN:{}', 212),\n",
       " ('а:CCONJ:cc:VERB:{};{}:VERB:conj:VERB:{}', 199),\n",
       " ('{}:NOUN:amod:ADJ:{}', 178),\n",
       " ('{}:VERB:nsubj:NOUN:{}', 163),\n",
       " ('а:CCONJ:cc:NOUN:{};{}:NOUN:conj:NOUN:{}', 157),\n",
       " ('{}:NOUN:conj:NOUN:{};{}:NOUN:cc:CCONJ:а', 152),\n",
       " ('{}:NOUN:nsubj:pass:VERB:{}', 141),\n",
       " ('{}:AUX:aux:VERB:{}', 122),\n",
       " ('{}:NOUN:acl:VERB:{}', 117),\n",
       " ('как:SCONJ:mark:VERB:{};{}:VERB:parataxis:VERB:{}', 101),\n",
       " ('{}:NOUN:case:ADP:в;в:ADP:fixed:NOUN:{}', 98),\n",
       " ('{}:VERB:acl:NOUN:{}', 94),\n",
       " ('{}:NOUN:nsubj:VERB:стать;стать:VERB:obl:NOUN:{}', 91),\n",
       " ('{}:VERB:nsubj:pass:NOUN:{}', 87),\n",
       " ('{}:PROPN:appos:NOUN:{}', 77),\n",
       " ('{}:VERB:case:ADP:в;в:ADP:fixed:NOUN:{}', 75),\n",
       " ('и:CCONJ:cc:ADJ:{};{}:ADJ:conj:ADJ:{}', 75),\n",
       " ('{}:NOUN:nmod:NOUN:результат;результат:NOUN:obl:VERB:{}', 74),\n",
       " ('{}:NOUN:nsubj:VERB:являться;являться:VERB:obl:NOUN:{}', 73),\n",
       " ('{}:VERB:obl:NOUN:{};{}:NOUN:det:DET:никакой', 69),\n",
       " ('{}:NOUN:nmod:NOUN:{};{}:NOUN:cc:CCONJ:и', 67),\n",
       " ('{}:VERB:csubj:VERB:{}', 65),\n",
       " ('{}:NOUN:appos:NOUN:аэропорт;аэропорт:NOUN:amod:ADJ:{}', 64),\n",
       " ('{}:ADJ:compound:ADJ:{}', 63),\n",
       " ('{}:NOUN:conj:NOUN:{};{}:NOUN:cc:CCONJ:или', 57),\n",
       " ('{}:NOUN:obl:VERB:стать;стать:VERB:nsubj:NOUN:{}', 52)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_counter.most_common(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_patterns = [pattern[0] for pattern in pattern_counter.most_common(500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/popular_patterns.csv', 'w') as pattern_file:\n",
    "    pattern_file.write('\\n'.join(popular_patterns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________\n",
    "\n",
    "Повторим эксперимент, учитывая сжатие CONJ рёбер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30394f0f9ef4b789946f4cc2cf30a88"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pattern_counter = Counter()\n",
    "\n",
    "no_deeppavlov = 0\n",
    "tree_failures = 0\n",
    "\n",
    "for filename in tqdm(file_list):\n",
    "    with open(filename, encoding='utf-8') as sentences_file:\n",
    "        sentences = json.load(sentences_file)\n",
    "        for sent in sentences:\n",
    "            if 'deeppavlov' not in sent:\n",
    "                no_deeppavlov += 1\n",
    "                continue\n",
    "            \n",
    "            multitokens, main_pos = sent['multi']\n",
    "            lemmas = sent['deeppavlov']\n",
    "            pos = sent['pos']\n",
    "            tree_info = sent['syntax']\n",
    "            \n",
    "            tree = SyntaxTree(empty=True)\n",
    "            tree.load_from_json(tree_info)\n",
    "            tree.compress_conj_edges()\n",
    "            \n",
    "            for pattern_pair in get_hypernymy_pairs(multitokens):\n",
    "                hypo_multi, hyper_multi = pattern_pair\n",
    "                hypo_main, hyper_main = main_pos[hypo_multi], main_pos[hyper_multi]\n",
    "                try:\n",
    "                    pattern = tree.get_syntax_pattern(hypo_main, hyper_main, pos, lemmas)\n",
    "                except:\n",
    "                    tree_failures += 1\n",
    "                    continue\n",
    "                    \n",
    "                if pattern is None:\n",
    "                    tree_failures += 1\n",
    "                    continue\n",
    "                pattern = ';'.join(pattern)\n",
    "                pattern_counter[pattern] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('{}:NOUN:nmod:NOUN:{}', 4684),\n",
       " ('{}:NOUN:appos:NOUN:{}', 1861),\n",
       " ('{}:NOUN:conj:NOUN:{};{}:NOUN:cc:CCONJ:и', 1181),\n",
       " ('{}:VERB:parataxis:VERB:{}', 1102),\n",
       " ('{}:NOUN:conj:NOUN:{}', 1073),\n",
       " ('{}:ADJ:amod:NOUN:{}', 1021),\n",
       " ('{}:NOUN:obl:VERB:{}', 1013),\n",
       " ('и:CCONJ:cc:NOUN:{};{}:NOUN:conj:NOUN:{}', 991),\n",
       " ('{}:VERB:conj:VERB:{}', 989),\n",
       " ('{}:VERB:advcl:VERB:{}', 846),\n",
       " ('{}:VERB:aux:AUX:{}', 768),\n",
       " ('{}:NOUN:parataxis:NOUN:{}', 728),\n",
       " ('{}:VERB:parataxis:VERB:{};{}:VERB:mark:SCONJ:как', 679),\n",
       " ('{}:VERB:obl:NOUN:{}', 598),\n",
       " ('{}:VERB:aux:pass:AUX:{}', 567),\n",
       " ('{}:VERB:ccomp:VERB:{}', 516),\n",
       " ('{}:VERB:conj:VERB:{};{}:VERB:cc:CCONJ:и', 507),\n",
       " ('и:CCONJ:cc:VERB:{};{}:VERB:conj:VERB:{}', 489),\n",
       " ('{}:VERB:xcomp:VERB:{}', 469),\n",
       " ('{}:VERB:obj:NOUN:{}', 385),\n",
       " ('{}:NOUN:nsubj:VERB:{}', 319),\n",
       " ('{}:NOUN:obj:VERB:{}', 308),\n",
       " ('{}:VERB:xcomp:ADJ:должный;должный:ADJ:cop:AUX:{}', 298),\n",
       " ('{}:VERB:conj:VERB:{};{}:VERB:cc:CCONJ:а', 255),\n",
       " ('{}:NOUN:nsubj:NOUN:{}', 221),\n",
       " ('а:CCONJ:cc:VERB:{};{}:VERB:conj:VERB:{}', 198),\n",
       " ('{}:NOUN:amod:ADJ:{}', 178),\n",
       " ('{}:VERB:nsubj:NOUN:{}', 165),\n",
       " ('а:CCONJ:cc:NOUN:{};{}:NOUN:conj:NOUN:{}', 156),\n",
       " ('{}:NOUN:conj:NOUN:{};{}:NOUN:cc:CCONJ:а', 152),\n",
       " ('и:CCONJ:cc:NOUN:{};{}:NOUN:nmod:NOUN:{}', 146),\n",
       " ('{}:NOUN:nsubj:pass:VERB:{}', 141),\n",
       " ('{}:AUX:aux:VERB:{}', 122),\n",
       " ('и:CCONJ:cc:NOUN:{};{}:NOUN:parataxis:NOUN:{}', 121),\n",
       " ('{}:NOUN:acl:VERB:{}', 119),\n",
       " ('как:SCONJ:mark:VERB:{};{}:VERB:parataxis:VERB:{}', 101),\n",
       " ('{}:NOUN:case:ADP:в;в:ADP:fixed:NOUN:{}', 98),\n",
       " ('{}:VERB:acl:NOUN:{}', 95),\n",
       " ('{}:NOUN:nsubj:VERB:стать;стать:VERB:obl:NOUN:{}', 92),\n",
       " ('{}:NOUN:nmod:NOUN:{};{}:NOUN:cc:CCONJ:и', 91),\n",
       " ('{}:VERB:nsubj:pass:NOUN:{}', 86),\n",
       " ('{}:NOUN:nsubj:VERB:являться;являться:VERB:obl:NOUN:{}', 77),\n",
       " ('{}:PROPN:appos:NOUN:{}', 77),\n",
       " ('{}:VERB:case:ADP:в;в:ADP:fixed:NOUN:{}', 75),\n",
       " ('и:CCONJ:cc:ADJ:{};{}:ADJ:conj:ADJ:{}', 75),\n",
       " ('{}:NOUN:nmod:NOUN:результат;результат:NOUN:obl:VERB:{}', 74),\n",
       " ('{}:VERB:obl:NOUN:{};{}:NOUN:det:DET:никакой', 69),\n",
       " ('{}:NOUN:appos:NOUN:аэропорт;аэропорт:NOUN:amod:ADJ:{}', 65),\n",
       " ('{}:VERB:csubj:VERB:{}', 65),\n",
       " ('{}:ADJ:compound:ADJ:{}', 63)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_counter.most_common(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что паттерны похожи на правду, и они более адекватные, чем те, которые были раньше. Однако самые частые из них состоят из всего одного перехода.\n",
    "\n",
    "Посмотрим на то, какие слова стоят рядом с гипонимом и гиперонимом. Ожидаем увидеть что-то вроде слов `такие`, `другие` или похожие."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypernym_neighbour_cnt = Counter()\n",
    "hyponym_neighbour_cnt = Counter()\n",
    "both_neighbour_cnt = Counter()\n",
    "\n",
    "def add_neighbours(lemmas, ind, ctr):\n",
    "    if ind > 0:\n",
    "        ctr[lemmas[ind - 1]] += 1\n",
    "    if ind + 1 < len(lemmas):\n",
    "        ctr[lemmas[ind + 1]] += 1\n",
    "\n",
    "for filename in tqdm(file_list):\n",
    "    with open(filename, encoding='utf-8') as sentences_file:\n",
    "        sentences = json.load(sentences_file)\n",
    "        for sent in sentences:\n",
    "            if 'deeppavlov' not in sent:\n",
    "                no_deeppavlov += 1\n",
    "                continue\n",
    "            \n",
    "            multitokens, main_pos = sent['multi']\n",
    "            lemmas = sent['deeppavlov']\n",
    "            pos = sent['pos']\n",
    "            tree_info = sent['syntax']\n",
    "            \n",
    "            tree = SyntaxTree(empty=True)\n",
    "            tree.load_from_json(tree_info)\n",
    "            \n",
    "            for pattern_pair in get_hypernymy_pairs(multitokens):\n",
    "                hypo_multi, hyper_multi = pattern_pair\n",
    "                hypo_main, hyper_main = main_pos[hypo_multi], main_pos[hyper_multi]\n",
    "                add_neighbours(lemmas, hypo_main, hyponym_neighbour_cnt)\n",
    "                add_neighbours(lemmas, hyper_main, hypernym_neighbour_cnt)\n",
    "                add_neighbours(lemmas, hypo_main, both_neighbour_cnt)\n",
    "                add_neighbours(lemmas, hyper_main, both_neighbour_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "both_neighbour_cnt.most_common(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно выделить следующие подходящие слова:\n",
    "\n",
    "`который`\n",
    "`должный`\n",
    "`также`\n",
    "`как`\n",
    "`тот`\n",
    "`весь`\n",
    "`другой`\n",
    "`такой`\n",
    "`несколько`\n",
    "`и`\n",
    "`или`\n",
    "`по`\n",
    "`никакой`\n",
    "`же`\n",
    "`ряд`\n",
    "`а`\n",
    "`так`\n",
    "`же`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако соседним словом в предложении может быть совершенно удалённая по дереву сущность, поэтому имеет смысл посмотреть на детей и родителя гипонима/гиперонима в синтаксическом дереве.\n",
    "Эту информацию вполне можно получить также из класса `SyntaxTree`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypernym_neighbour_cnt = Counter()\n",
    "hyponym_neighbour_cnt = Counter()\n",
    "both_neighbour_cnt = Counter()\n",
    "\n",
    "def add_neighbours(tree, lemma, ind, ctr):\n",
    "    for child in tree.children[ind]:\n",
    "        ctr[lemma[child]] += 1\n",
    "\n",
    "for filename in tqdm(file_list):\n",
    "    with open(filename, encoding='utf-8') as sentences_file:\n",
    "        sentences = json.load(sentences_file)\n",
    "        for sent in sentences:\n",
    "            if 'deeppavlov' not in sent:\n",
    "                no_deeppavlov += 1\n",
    "                continue\n",
    "            \n",
    "            multitokens, main_pos = sent['multi']\n",
    "            lemmas = sent['deeppavlov']\n",
    "            pos = sent['pos']\n",
    "            tree_info = sent['syntax']\n",
    "            \n",
    "            tree = SyntaxTree(empty=True)\n",
    "            tree.load_from_json(tree_info)\n",
    "            \n",
    "            for pattern_pair in get_hypernymy_pairs(multitokens):\n",
    "                hypo_multi, hyper_multi = pattern_pair\n",
    "                hypo_main, hyper_main = main_pos[hypo_multi], main_pos[hyper_multi]\n",
    "                add_neighbours(tree, lemmas, hypo_main, hyponym_neighbour_cnt)\n",
    "                add_neighbours(tree, lemmas, hyper_main, hypernym_neighbour_cnt)\n",
    "                add_neighbours(tree, lemmas, hypo_main, both_neighbour_cnt)\n",
    "                add_neighbours(tree, lemmas, hyper_main, both_neighbour_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_neighbour_cnt.most_common(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Изменение алгоритма в построении паттернов**\n",
    "\n",
    "Данные уже почище, но тем не менее новых адекватных слов для добавления нет. Так что алгоритм добавления примерно следующий:\n",
    "\n",
    "* смотрим слово, от которого требуется построить паттерн\n",
    "\n",
    "* смотрим на его детей. если среди них есть слово из выделенных, то строим паттерн от него\n",
    "\n",
    "* проверяем только, что это слово уже не является соседом в пути"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
