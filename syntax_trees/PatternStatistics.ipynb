{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from thesaurus_parsing.thesaurus_parser import ThesaurusParser\n",
    "from syntax_tree import SyntaxTree\n",
    "from os.path import join\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение паттернов гиперонимии\n",
    "\n",
    "Воспользуемся тем, что теперь все данные лежат в удобном формате. Построим тезаурус и будем искать паттерны гиперонимии.\n",
    "\n",
    "Сначала прости считаем имеющийся список файлов с обработанными текстами, которые имеются на данный момент (пока что идёт обработка 36к документов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH = \"/home/loginov-ra/MIPT/HypernymyDetection/data/Lenta/texts_tagged_processed_tree\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir(DIR_PATH)\n",
    "file_list = [join(DIR_PATH, filename) for filename in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36437"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/loginov-ra/MIPT/HypernymyDetection/data/Lenta/texts_tagged_processed_tree/20100603protes.txt_processed.json'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В каждом файле известен набор синтаксических деревьев и лемматизированных предложений. Загрузим словарь гиперонимов и посмотрим, как проверять в нём наличие отношения гиперонимии между двумя заданными сущностями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesaurus = ThesaurusParser(\"../data/RuThes\", need_closure=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['макроорганизм',\n",
       " 'организм',\n",
       " 'живой организм',\n",
       " 'особь',\n",
       " 'существо',\n",
       " 'живность',\n",
       " 'индивидуум',\n",
       " 'биологический',\n",
       " 'биологический организм',\n",
       " 'субъект деятельность',\n",
       " 'живое',\n",
       " 'живой существо']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thesaurus.hypernyms_dict['человек']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем проходить по каждому предложению, искать сущности, которые являются гиперонимами и строить синтаксический паттерн из записанного в данных дерева. По каждому паттерну заведём статистику встречаемости.\n",
    "\n",
    "**Важно.** Эту процедуру надо производить только на обучающей части данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_hyponym_hypernym(hypo_cand, hyper_cand):\n",
    "    if hypo_cand not in thesaurus.hypernyms_dict:\n",
    "        return False\n",
    "    return hyper_cand in thesaurus.hypernyms_dict[hypo_cand]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hypernymy_pairs(multitokens):\n",
    "    pairs = []\n",
    "    for i, hypernym_candidate in enumerate(multitokens):\n",
    "        for j, hyponym_candidate in enumerate(multitokens):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if is_hyponym_hypernym(hyponym_candidate, hypernym_candidate):\n",
    "                pairs.append((j, i))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7813248c364140bb1d8f0e34a8e0d0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pattern_counter = Counter()\n",
    "\n",
    "no_deeppavlov = 0\n",
    "tree_failures = 0\n",
    "\n",
    "for filename in tqdm(file_list):\n",
    "    with open(filename, encoding='utf-8') as sentences_file:\n",
    "        sentences = json.load(sentences_file)\n",
    "        for sent in sentences:\n",
    "            if 'deeppavlov' not in sent:\n",
    "                no_deeppavlov += 1\n",
    "                continue\n",
    "            \n",
    "            multitokens, main_pos = sent['multi']\n",
    "            lemmas = sent['deeppavlov']\n",
    "            pos = sent['pos']\n",
    "            tree_info = sent['syntax']\n",
    "            \n",
    "            tree = SyntaxTree(empty=True)\n",
    "            tree.load_from_json(tree_info)\n",
    "            \n",
    "            for pattern_pair in get_hypernymy_pairs(multitokens):\n",
    "                hypo_multi, hyper_multi = pattern_pair\n",
    "                hypo_main, hyper_main = main_pos[hypo_multi], main_pos[hyper_multi]\n",
    "                try:\n",
    "                    pattern = tree.get_syntax_pattern(hypo_main, hyper_main, pos, lemmas)\n",
    "                except:\n",
    "                    tree_failures += 1\n",
    "                    continue\n",
    "                    \n",
    "                if pattern is None:\n",
    "                    tree_failures += 1\n",
    "                    continue\n",
    "                pattern = ';'.join(pattern)\n",
    "                pattern_counter[pattern] += 1\n",
    "                \n",
    "                #if pattern == '{}:NOUN:conj:NOUN:{}':\n",
    "                #    print(' '.join(sent['initial']))\n",
    "                #    print(multitokens[hypo_multi], multitokens[hyper_multi])\n",
    "                #    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('{}:NOUN:nmod:NOUN:{}', 4641),\n",
       " ('{}:NOUN:appos:NOUN:{}', 1805),\n",
       " ('{}:NOUN:conj:NOUN:{};{}:NOUN:cc:CCONJ:и', 1179),\n",
       " ('{}:VERB:parataxis:VERB:{}', 1079),\n",
       " ('{}:NOUN:conj:NOUN:{}', 1073),\n",
       " ('{}:ADJ:amod:NOUN:{}', 1015),\n",
       " ('{}:NOUN:obl:VERB:{}', 1000),\n",
       " ('{}:VERB:conj:VERB:{}', 987),\n",
       " ('и:CCONJ:cc:NOUN:{};{}:NOUN:conj:NOUN:{}', 985),\n",
       " ('{}:VERB:advcl:VERB:{}', 842)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_counter.most_common(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_patterns = [pattern[0] for pattern in pattern_counter.most_common(500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/popular_patterns.csv', 'w') as pattern_file:\n",
    "    pattern_file.write('\\n'.join(popular_patterns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что паттерны похожи на правду, и они более адекватные, чем те, которые были раньше. Однако самые частые из них состоят из всего одного перехода.\n",
    "\n",
    "Посмотрим на то, какие слова стоят рядом с гипонимом и гиперонимом. Ожидаем увидеть что-то вроде слов `такие`, `другие` или похожие."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baab7d5dcd2641008bc675bb43e88b60"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3f4f55d5bbf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mpattern_pair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_hypernymy_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultitokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0mhypo_multi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper_multi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern_pair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mhypo_main\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper_main\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_pos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhypo_multi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_pos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhyper_multi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-8608dd1487db>\u001b[0m in \u001b[0;36mget_hypernymy_pairs\u001b[0;34m(multitokens)\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mis_hyponym_hypernym\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyponym_candidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypernym_candidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                 \u001b[0mpairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-4fd2a467b0dd>\u001b[0m in \u001b[0;36mis_hyponym_hypernym\u001b[0;34m(hypo_cand, hyper_cand)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_hyponym_hypernym\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypo_cand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper_cand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mhypo_cand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthesaurus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypernyms_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhyper_cand\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthesaurus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypernyms_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhypo_cand\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hypernym_neighbour_cnt = Counter()\n",
    "hyponym_neighbour_cnt = Counter()\n",
    "both_neighbour_cnt = Counter()\n",
    "\n",
    "def add_neighbours(lemmas, ind, ctr):\n",
    "    if ind > 0:\n",
    "        ctr[lemmas[ind - 1]] += 1\n",
    "    if ind + 1 < len(lemmas):\n",
    "        ctr[lemmas[ind + 1]] += 1\n",
    "\n",
    "for filename in tqdm(file_list):\n",
    "    with open(filename, encoding='utf-8') as sentences_file:\n",
    "        sentences = json.load(sentences_file)\n",
    "        for sent in sentences:\n",
    "            if 'deeppavlov' not in sent:\n",
    "                no_deeppavlov += 1\n",
    "                continue\n",
    "            \n",
    "            multitokens, main_pos = sent['multi']\n",
    "            lemmas = sent['deeppavlov']\n",
    "            pos = sent['pos']\n",
    "            tree_info = sent['syntax']\n",
    "            \n",
    "            tree = SyntaxTree(empty=True)\n",
    "            tree.load_from_json(tree_info)\n",
    "            \n",
    "            for pattern_pair in get_hypernymy_pairs(multitokens):\n",
    "                hypo_multi, hyper_multi = pattern_pair\n",
    "                hypo_main, hyper_main = main_pos[hypo_multi], main_pos[hyper_multi]\n",
    "                add_neighbours(lemmas, hypo_main, hyponym_neighbour_cnt)\n",
    "                add_neighbours(lemmas, hyper_main, hypernym_neighbour_cnt)\n",
    "                add_neighbours(lemmas, hypo_main, both_neighbour_cnt)\n",
    "                add_neighbours(lemmas, hyper_main, both_neighbour_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 3503),\n",
       " ('в', 2618),\n",
       " ('\"', 1420),\n",
       " ('и', 1254),\n",
       " ('.', 1155),\n",
       " ('на', 1006),\n",
       " ('быть', 790),\n",
       " ('с', 743),\n",
       " ('не', 555),\n",
       " ('по', 520)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "both_neighbour_cnt.most_common(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно выделить следующие подходящие слова:\n",
    "\n",
    "`который`\n",
    "`должный`\n",
    "`также`\n",
    "`как`\n",
    "`тот`\n",
    "`весь`\n",
    "`другой`\n",
    "`такой`\n",
    "`несколько`\n",
    "`и`\n",
    "`или`\n",
    "`по`\n",
    "`никакой`\n",
    "`же`\n",
    "`ряд`\n",
    "`а`\n",
    "`так`\n",
    "`же`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако соседним словом в предложении может быть совершенно удалённая по дереву сущность, поэтому имеет смысл посмотреть на детей и родителя гипонима/гиперонима в синтаксическом дереве.\n",
    "Эту информацию вполне можно получить также из класса `SyntaxTree`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569d10a6c11240a3806e75cba9b26a57"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hypernym_neighbour_cnt = Counter()\n",
    "hyponym_neighbour_cnt = Counter()\n",
    "both_neighbour_cnt = Counter()\n",
    "\n",
    "def add_neighbours(tree, lemma, ind, ctr):\n",
    "    for child in tree.children[ind]:\n",
    "        ctr[lemma[child]] += 1\n",
    "\n",
    "for filename in tqdm(file_list):\n",
    "    with open(filename, encoding='utf-8') as sentences_file:\n",
    "        sentences = json.load(sentences_file)\n",
    "        for sent in sentences:\n",
    "            if 'deeppavlov' not in sent:\n",
    "                no_deeppavlov += 1\n",
    "                continue\n",
    "            \n",
    "            multitokens, main_pos = sent['multi']\n",
    "            lemmas = sent['deeppavlov']\n",
    "            pos = sent['pos']\n",
    "            tree_info = sent['syntax']\n",
    "            \n",
    "            tree = SyntaxTree(empty=True)\n",
    "            tree.load_from_json(tree_info)\n",
    "            \n",
    "            for pattern_pair in get_hypernymy_pairs(multitokens):\n",
    "                hypo_multi, hyper_multi = pattern_pair\n",
    "                hypo_main, hyper_main = main_pos[hypo_multi], main_pos[hyper_multi]\n",
    "                add_neighbours(tree, lemmas, hypo_main, hyponym_neighbour_cnt)\n",
    "                add_neighbours(tree, lemmas, hyper_main, hypernym_neighbour_cnt)\n",
    "                add_neighbours(tree, lemmas, hypo_main, both_neighbour_cnt)\n",
    "                add_neighbours(tree, lemmas, hyper_main, both_neighbour_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 3717),\n",
       " (',', 3630),\n",
       " ('в', 1916),\n",
       " ('\"', 1100),\n",
       " ('и', 947),\n",
       " ('быть', 639),\n",
       " ('на', 569),\n",
       " ('что', 500),\n",
       " ('по', 402),\n",
       " ('который', 394),\n",
       " ('тот', 364),\n",
       " ('год', 324),\n",
       " ('с', 323),\n",
       " ('должный', 311),\n",
       " ('не', 308),\n",
       " ('-', 283),\n",
       " ('один', 259),\n",
       " ('как', 253),\n",
       " ('он', 236),\n",
       " ('это', 226),\n",
       " ('мочь', 225),\n",
       " ('россия', 211),\n",
       " ('заявить', 206),\n",
       " ('а', 206),\n",
       " ('принять', 205),\n",
       " ('из', 195),\n",
       " ('этот', 184),\n",
       " ('сообщать', 183),\n",
       " ('о', 180),\n",
       " ('стать', 177),\n",
       " ('(', 171),\n",
       " (')', 171),\n",
       " ('компания', 167),\n",
       " ('--', 165),\n",
       " ('провести', 160),\n",
       " ('получить', 157),\n",
       " ('находиться', 136),\n",
       " ('к', 134),\n",
       " ('для', 131),\n",
       " ('процент', 127),\n",
       " ('являться', 126),\n",
       " ('два', 126),\n",
       " ('они', 125),\n",
       " ('человек', 123),\n",
       " ('решение', 122),\n",
       " ('работа', 117),\n",
       " ('территория', 111),\n",
       " ('система', 111),\n",
       " ('страна', 110),\n",
       " ('дело', 109)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "both_neighbour_cnt.most_common(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Изменение алгоритма в построении паттернов**\n",
    "\n",
    "Данные уже почище, но тем не менее новых адекватных слов для добавления нет. Так что алгоритм добавления примерно следующий:\n",
    "\n",
    "* смотрим слово, от которого требуется построить паттерн\n",
    "\n",
    "* смотрим на его детей. если среди них есть слово из выделенных, то строим паттерн от него\n",
    "\n",
    "* проверяем только, что это слово уже не является соседом в пути"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
