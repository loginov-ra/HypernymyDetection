{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение матриц проекции\n",
    "\n",
    "Реализуем процесс обучения матриц проекции, который был предложен на SemEval CRIM. Для этого сначала требуется загрузить какие-нибудь эмбеддинги для слов и подобрать то, каких именно кандидатов требуется ранжировать при выдаче гиперонимов.\n",
    "\n",
    "Начнём с простой стратегии. Возьмём обученную модель FastText и загрузим эмбеддинги из неё. После этого будем для многословных термов усреднять эмбеддинги."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from os.path import join\n",
    "import os\n",
    "sys.path.append(\"../\")\n",
    "import fasttext as ft\n",
    "from thesaurus_parsing.thesaurus_parser import ThesaurusParser\n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "deeppavlov_embeddings = ft.load_model('../data/models/fasttext_deeppavlov.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1572343"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(deeppavlov_embeddings.get_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что здесь есть векторы для полутора миллионов слов. Конечно же, использовать их все, как кандидаты в гиперонимы, было бы хорошо. Но тем не менее, поскольку метод в основном похож на kNN, это будет очень долго.\n",
    "\n",
    "Вследствие этого, необходимо, кроме сущностей тезауруса, оставить лишь некоторый топ в качестве кандидатов в гиперонимы. Этот топ можно подобрать по tf-idf. Но для начала надо построить векторы для сущностей из тезауруса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesaurus = ThesaurusParser(\"../data/RuThes\", need_closure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_embeddings = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, entry_dict in thesaurus.text_entries.items():\n",
    "    lemma = entry_dict['lemma']\n",
    "    vocab_embeddings[lemma] = deeppavlov_embeddings.get_sentence_vector(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110176"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте пройдём по всем текстам, которые загрузились на данный момент, для слов, которые есть в словаре, посчитаем частоту слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH = \"/home/loginov-ra/MIPT/HypernymyDetection/data/Lenta/texts_tagged_processed_tree\"\n",
    "file_list = os.listdir(DIR_PATH)\n",
    "file_list = [join(DIR_PATH, filename) for filename in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23b21a6e26648f9a50768e82a0a0fb5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word_ctr = Counter()\n",
    "no_deeppavlov = 0\n",
    "\n",
    "for filename in tqdm(file_list):\n",
    "    with open(filename, encoding='utf-8') as sentences_file:\n",
    "        sentences = json.load(sentences_file)\n",
    "        for sent in sentences:\n",
    "            if 'deeppavlov' not in sent:\n",
    "                no_deeppavlov += 1\n",
    "                continue\n",
    "            \n",
    "            multitokens, _ = sent['multi']\n",
    "            for t in multitokens:\n",
    "                word_ctr[t] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "883\n"
     ]
    }
   ],
   "source": [
    "print(no_deeppavlov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 521733),\n",
       " ('.', 445009),\n",
       " ('в', 307206),\n",
       " ('\"', 299914),\n",
       " ('на', 125625),\n",
       " ('и', 123598),\n",
       " ('-', 111278),\n",
       " ('с', 77467),\n",
       " ('что', 72277),\n",
       " ('быть', 70839),\n",
       " ('по', 70425),\n",
       " ('год', 57392),\n",
       " ('о', 52364),\n",
       " (')', 49128),\n",
       " ('(', 48432),\n",
       " ('не', 47363),\n",
       " ('который', 42268),\n",
       " ('он', 40684),\n",
       " (':', 40304),\n",
       " ('это', 37290),\n",
       " ('из', 37212),\n",
       " ('тот', 32196),\n",
       " ('за', 28677),\n",
       " ('как', 28572),\n",
       " ('один', 27042),\n",
       " ('--', 26014),\n",
       " ('к', 23407),\n",
       " ('сообщать', 22883)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ctr.most_common(n=28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что для первых $27$ слов нет необходимости искать гиперонимы, для остальных уже может быть. Поэтому возьмёи пока первые $100000$ слов для работы с ними, посчитаем их эмбеддинги и добавим в словарь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_words = word_ctr.most_common(n=100000)[27:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, _ in additional_words:\n",
    "    vocab_embeddings[word] = deeppavlov_embeddings.get_word_vector(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим модель за ненадобностью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del deeppavlov_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172251"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что было пересечение, и добавилась где-то 61000 слов\n",
    "_________________\n",
    "\n",
    "**Определение модели**\n",
    "\n",
    "Определим модель, в которой будет 5 матриц проекции и логистическая регрессия на косинусных расстояниях до проекций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRIMModel(nn.Module):\n",
    "    def __init__(self, n_matrices=5, embedding_dim=300, init_sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_matrices = n_matrices\n",
    "        self.init_sigma = init_sigma\n",
    "        \n",
    "        matrix_shape = (n_matrices, 1, embedding_dim, embedding_dim)\n",
    "        self.matrices = torch.FloatTensor(size=matrix_shape)\n",
    "        self.prob_layer = nn.Linear(in_features=n_matrices, out_features=1)\n",
    "        \n",
    "        for i in range(n_matrices):\n",
    "            eye_tensor = torch.FloatTensor(size=(embedding_dim, embedding_dim), device=device)\n",
    "            noise_tensor = torch.FloatTensor(size=(embedding_dim, embedding_dim), device=device)\n",
    "            torch.nn.init.eye_(eye_tensor)\n",
    "            torch.nn.init.normal_(noise_tensor, std=init_sigma)\n",
    "            self.matrices[i][0] = eye_tensor + noise_tensor\n",
    "            \n",
    "        torch.nn.init.normal_(self.prob_layer.weight, std=0.1)\n",
    "        self.matrices = nn.Parameter(self.matrices.requires_grad_())\n",
    "        \n",
    "    def forward(self, input_dict):\n",
    "        candidate = input_dict['candidate']\n",
    "        candidate_batch = candidate.shape[0]\n",
    "        candidate = candidate.view((candidate_batch, 1, self.embedding_dim))\n",
    "        batch = input_dict['batch'].unsqueeze(-1)\n",
    "        batch_size = batch.shape[0]\n",
    "        projections = torch.matmul(self.matrices, batch).permute(1, 0, 2, 3).squeeze(-1)\n",
    "        similarities = F.cosine_similarity(projections, candidate, dim=-1)\n",
    "        logits = self.prob_layer(similarities)\n",
    "        probas = torch.sigmoid(logits)\n",
    "        return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CRIMModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'batch': torch.randn(64, 300),\n",
    "    'candidate': torch.randn(1, 300)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 35.45905\n",
      "Loss: 9.963653\n",
      "Loss: 0.77415305\n",
      "Loss: 0.17260851\n",
      "Loss: 0.056600925\n",
      "Loss: 0.02204978\n",
      "Loss: 0.0098106\n",
      "Loss: 0.0048678904\n",
      "Loss: 0.0026406413\n",
      "Loss: 0.0015418465\n",
      "Loss: 0.0009574688\n",
      "Loss: 0.00062648166\n",
      "Loss: 0.000428732\n",
      "Loss: 0.00030505323\n",
      "Loss: 0.00022457582\n",
      "Loss: 0.00017036783\n",
      "Loss: 0.00013272851\n",
      "Loss: 0.00010588361\n",
      "Loss: 8.6275795e-05\n",
      "Loss: 7.164689e-05\n",
      "Loss: 6.0523584e-05\n",
      "Loss: 5.1920768e-05\n",
      "Loss: 4.5164634e-05\n",
      "Loss: 3.9785413e-05\n",
      "Loss: 3.5448786e-05\n",
      "Loss: 3.191338e-05\n",
      "Loss: 2.9001752e-05\n",
      "Loss: 2.6581678e-05\n",
      "Loss: 2.4553427e-05\n",
      "Loss: 2.28407e-05\n",
      "Loss: 2.1384463e-05\n",
      "Loss: 2.0138586e-05\n",
      "Loss: 1.9066734e-05\n",
      "Loss: 1.813982e-05\n",
      "Loss: 1.7334512e-05\n",
      "Loss: 1.6631917e-05\n",
      "Loss: 1.601655e-05\n",
      "Loss: 1.5475733e-05\n",
      "Loss: 1.4998867e-05\n",
      "Loss: 1.4577208e-05\n",
      "Loss: 1.4203378e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-86fa3c98454d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    probas = model(args)\n",
    "    loss = probas.sum()\n",
    "    print('Loss:', loss.detach().numpy())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во всяком случае, на текущий момент возможно переобучить модель под нужные значения\n",
    "_________________\n",
    "\n",
    "**Цикл обучения модели**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы обучить модель, надо сделать следующие шаги:\n",
    "\n",
    "* Добавить пары корректных гипонимов-гиперонимов\n",
    "* Для каждого положительного добавить несколько отрицательных (пока просто случайные слова)\n",
    "* Добавить вектор правильных ответов\n",
    "\n",
    "Сделаем из этого `torch.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HypernymQueriesDataset(Dataset):\n",
    "    def load_train_items(self, thesaurus, vocab):\n",
    "        self.train_items = []\n",
    "        for hyponym, hypernyms in tqdm(thesaurus.hypernyms_dict.items()):\n",
    "            for hypernym in hypernyms:\n",
    "                self.train_items.append([hyponym, hypernym, True])\n",
    "            \n",
    "            negative_examples = np.random.choice(list(vocab.keys()), size=self.n_negative * len(hypernyms))\n",
    "            for negative in negative_examples:\n",
    "                self.train_items.append([hyponym, negative, False])\n",
    "                \n",
    "            if self.max_pairs is not None and len(self.train_items) > self.max_pairs:\n",
    "                break\n",
    "    \n",
    "    def __init__(self, thesaurus, vocab, n_negative=3, max_pairs=None):\n",
    "        self.n_negative = n_negative\n",
    "        self.thesaurus = thesaurus\n",
    "        self.vocab = vocab\n",
    "        self.max_pairs = max_pairs\n",
    "        \n",
    "        self.load_train_items(thesaurus, vocab)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_items)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        hyponym, hypernym, label = self.train_items[idx]\n",
    "        return (self.vocab[hyponym], self.vocab[hypernym], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7f1c44db2d4de9acf1eff9a55f3af9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = HypernymQueriesDataset(thesaurus, vocab_embeddings, max_pairs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CRIMModel(n_matrices=24)\n",
    "optimizer = Adam(model.parameters(), lr=3e-4)\n",
    "n_epochs = 1\n",
    "batch_size = 64\n",
    "plot_frequency = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Started training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a42ade41df4466a0e56fa2c05c1594"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_history = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch {}. Started training.'.format(epoch + 1))\n",
    "\n",
    "    for it, batch in tqdm(enumerate(data_loader), total=len(dataset) / batch_size):\n",
    "        hyponyms, candidates, labels = batch\n",
    "\n",
    "        model_batch = {\n",
    "            'batch': hyponyms,\n",
    "            'candidate': candidates\n",
    "        }\n",
    "\n",
    "        labels = labels.float()\n",
    "        probas = model(model_batch).squeeze()\n",
    "        probas = torch.clamp(probas, 1e-5, 1. - 1e-5)\n",
    "        loss = torch.sum(labels * torch.log(probas) + (1. - labels) * torch.log(1. - probas))\n",
    "        loss_history.append(loss.detach().numpy())\n",
    "        \n",
    "        if len(loss_history) % plot_frequency == 0:\n",
    "            print(loss)\n",
    "            clear_output(wait=True)\n",
    "            plt.figure(figsize=(12, 7))\n",
    "            plt.title('Loss history', fontsize=18)\n",
    "            plt.xlabel('Iteration', fontsize=15)\n",
    "            plt.ylabel('CE loss', fontsize=15)\n",
    "            plt.plot(np.array(loss_history))\n",
    "            plt.show()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../data/models/projection_model.bin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
