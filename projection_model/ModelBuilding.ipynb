{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение матриц проекции\n",
    "\n",
    "Реализуем процесс обучения матриц проекции, который был предложен на SemEval CRIM. Для этого сначала требуется загрузить какие-нибудь эмбеддинги для слов и подобрать то, каких именно кандидатов требуется ранжировать при выдаче гиперонимов.\n",
    "\n",
    "Начнём с простой стратегии. Возьмём обученную модель FastText и загрузим эмбеддинги из неё. После этого будем для многословных термов усреднять эмбеддинги."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from os.path import join\n",
    "import os\n",
    "sys.path.append(\"../\")\n",
    "import fasttext as ft\n",
    "from thesaurus_parsing.thesaurus_parser import ThesaurusParser\n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "deeppavlov_embeddings = ft.load_model('../data/models/fasttext_deeppavlov.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1572343"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(deeppavlov_embeddings.get_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что здесь есть векторы для полутора миллионов слов. Конечно же, использовать их все, как кандидаты в гиперонимы, было бы хорошо. Но тем не менее, поскольку метод в основном похож на kNN, это будет очень долго.\n",
    "\n",
    "Вследствие этого, необходимо, кроме сущностей тезауруса, оставить лишь некоторый топ в качестве кандидатов в гиперонимы. Этот топ можно подобрать по tf-idf. Но для начала надо построить векторы для сущностей из тезауруса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesaurus = ThesaurusParser(\"../data/RuThes\", need_closure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_embeddings = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, entry_dict in thesaurus.text_entries.items():\n",
    "    lemma = entry_dict['lemma']\n",
    "    vocab_embeddings[lemma] = deeppavlov_embeddings.get_sentence_vector(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110176"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте пройдём по всем текстам, которые загрузились на данный момент, для слов, которые есть в словаре, посчитаем частоту слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH = \"/home/loginov-ra/MIPT/HypernymyDetection/data/Lenta/texts_tagged_processed_tree\"\n",
    "file_list = os.listdir(DIR_PATH)\n",
    "file_list = [join(DIR_PATH, filename) for filename in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec48b01b2294210890ae52205f30624"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word_ctr = Counter()\n",
    "no_deeppavlov = 0\n",
    "\n",
    "for filename in tqdm(file_list):\n",
    "    with open(filename, encoding='utf-8') as sentences_file:\n",
    "        sentences = json.load(sentences_file)\n",
    "        for sent in sentences:\n",
    "            if 'deeppavlov' not in sent:\n",
    "                no_deeppavlov += 1\n",
    "                continue\n",
    "            \n",
    "            multitokens, _ = sent['multi']\n",
    "            for t in multitokens:\n",
    "                word_ctr[t] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "883\n"
     ]
    }
   ],
   "source": [
    "print(no_deeppavlov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 521733),\n",
       " ('.', 445009),\n",
       " ('в', 307206),\n",
       " ('\"', 299914),\n",
       " ('на', 125625),\n",
       " ('и', 123598),\n",
       " ('-', 111278),\n",
       " ('с', 77467),\n",
       " ('что', 72277),\n",
       " ('быть', 70839),\n",
       " ('по', 70425),\n",
       " ('год', 57392),\n",
       " ('о', 52364),\n",
       " (')', 49128),\n",
       " ('(', 48432),\n",
       " ('не', 47363),\n",
       " ('который', 42268),\n",
       " ('он', 40684),\n",
       " (':', 40304),\n",
       " ('это', 37290),\n",
       " ('из', 37212),\n",
       " ('тот', 32196),\n",
       " ('за', 28677),\n",
       " ('как', 28572),\n",
       " ('один', 27042),\n",
       " ('--', 26014),\n",
       " ('к', 23407),\n",
       " ('сообщать', 22883)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ctr.most_common(n=28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что для первых $27$ слов нет необходимости искать гиперонимы, для остальных уже может быть. Поэтому возьмёи пока первые $100000$ слов для работы с ними, посчитаем их эмбеддинги и добавим в словарь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_words = word_ctr.most_common(n=100000)[27:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, _ in additional_words:\n",
    "    vocab_embeddings[word] = deeppavlov_embeddings.get_word_vector(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим модель за ненадобностью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del deeppavlov_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172251"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что было пересечение, и добавилась где-то 61000 слов\n",
    "_________________\n",
    "\n",
    "**Определение модели**\n",
    "\n",
    "Определим модель, в которой будет 5 матриц проекции и логистическая регрессия на косинусных расстояниях до проекций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRIMModel(nn.Module):\n",
    "    def __init__(self, n_matrices=5, embedding_dim=300, init_sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_matrices = n_matrices\n",
    "        self.init_sigma = init_sigma\n",
    "        \n",
    "        matrix_shape = (n_matrices, 1, embedding_dim, embedding_dim)\n",
    "        self.matrices = torch.FloatTensor(size=matrix_shape)\n",
    "        self.prob_layer = nn.Linear(in_features=n_matrices, out_features=1)\n",
    "        \n",
    "        for i in range(n_matrices):\n",
    "            eye_tensor = torch.FloatTensor(size=(embedding_dim, embedding_dim), device=device)\n",
    "            noise_tensor = torch.FloatTensor(size=(embedding_dim, embedding_dim), device=device)\n",
    "            torch.nn.init.eye_(eye_tensor)\n",
    "            torch.nn.init.normal_(noise_tensor, std=init_sigma)\n",
    "            self.matrices[i][0] = eye_tensor + noise_tensor\n",
    "            \n",
    "        torch.nn.init.normal_(self.prob_layer.weight, std=0.1)\n",
    "        self.matrices = nn.Parameter(self.matrices.requires_grad_())\n",
    "        \n",
    "    def forward(self, input_dict):\n",
    "        candidate = input_dict['candidate']\n",
    "        candidate_batch = candidate.shape[0]\n",
    "        candidate = candidate.view((candidate_batch, 1, self.embedding_dim))\n",
    "        batch = input_dict['batch'].unsqueeze(-1)\n",
    "        \n",
    "        print('Batch')\n",
    "        print(batch)\n",
    "        \n",
    "        print('Matrices')\n",
    "        print(model.matrices)\n",
    "        \n",
    "        batch_size = batch.shape[0]\n",
    "        projections = torch.matmul(self.matrices, batch).permute(1, 0, 2, 3).squeeze(-1)\n",
    "        \n",
    "        print('Projections')\n",
    "        print(projections)\n",
    "        \n",
    "        similarities = F.cosine_similarity(projections, candidate, dim=-1)\n",
    "        \n",
    "        print('Similarities')\n",
    "        print(similarities)\n",
    "        \n",
    "        logits = self.prob_layer(similarities)\n",
    "        \n",
    "        print('Logits')\n",
    "        print(logits)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CRIMModel(n_matrices=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'batch': torch.randn(64, 300),\n",
    "    'candidate': torch.randn(1, 300)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch\n",
      "tensor([[[ 0.7252664566],\n",
      "         [-0.8776559830],\n",
      "         [ 0.1537696570],\n",
      "         ...,\n",
      "         [ 0.6769005060],\n",
      "         [ 1.9011576176],\n",
      "         [-0.3448872566]],\n",
      "\n",
      "        [[ 0.3573443890],\n",
      "         [ 1.1088384390],\n",
      "         [ 0.3916432261],\n",
      "         ...,\n",
      "         [ 0.9089306593],\n",
      "         [-0.0138565507],\n",
      "         [-0.2266946733]],\n",
      "\n",
      "        [[-1.6157082319],\n",
      "         [ 0.4387985170],\n",
      "         [-2.3074474335],\n",
      "         ...,\n",
      "         [-0.3596394658],\n",
      "         [-0.1646138281],\n",
      "         [-1.9526571035]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2422087938],\n",
      "         [-0.5033995509],\n",
      "         [-0.8423961997],\n",
      "         ...,\n",
      "         [-0.0900238976],\n",
      "         [-0.2999053299],\n",
      "         [-0.2994983494]],\n",
      "\n",
      "        [[ 0.6062324047],\n",
      "         [-2.6377589703],\n",
      "         [ 2.6361970901],\n",
      "         ...,\n",
      "         [-1.3261247873],\n",
      "         [ 0.9108996391],\n",
      "         [ 1.2913289070]],\n",
      "\n",
      "        [[-0.6621691585],\n",
      "         [-0.6577079296],\n",
      "         [ 0.4029124081],\n",
      "         ...,\n",
      "         [ 0.3069745600],\n",
      "         [-0.5925599337],\n",
      "         [-1.0219676495]]])\n",
      "Matrices\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.4020110965,  0.0377818160, -0.0768745095,  ...,\n",
      "            0.0637209192, -0.1196846366,  0.0218704175],\n",
      "          [ 0.0867496133,  0.5100136399, -0.0653859526,  ...,\n",
      "           -0.0700912699, -0.1762113720,  0.1514299810],\n",
      "          [-0.0283051133, -0.0778824091,  0.2792578936,  ...,\n",
      "            0.0153787080,  0.0420851335,  0.1609983444],\n",
      "          ...,\n",
      "          [-0.0298963040, -0.3274365664,  0.0402783677,  ...,\n",
      "            0.2811460793,  0.1933815926, -0.0402422957],\n",
      "          [-0.1070354432, -0.3695237339,  0.1826210171,  ...,\n",
      "           -0.0642767474,  0.8497835994,  0.0615076460],\n",
      "          [ 0.0008602891, -0.2091080248,  0.0068247169,  ...,\n",
      "           -0.0585349388, -0.0505063720,  0.2152075917]]]], requires_grad=True)\n",
      "Projections\n",
      "tensor([[[ 3.6808226109, -0.1890701056, -0.0875727236,  ...,\n",
      "          -2.0722196102, -3.6707925797, -0.8684788346]],\n",
      "\n",
      "        [[ 0.1795753837, -1.2978434563,  1.8087451458,  ...,\n",
      "           5.2709798813,  5.0546050072,  1.4197984934]],\n",
      "\n",
      "        [[-1.4446461201, -1.0291117430, -0.8918651342,  ...,\n",
      "          -1.9395866394,  0.5402700901, -1.8757467270]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.8338177204,  0.7319726348, -3.4426529408,  ...,\n",
      "          -7.5526313782, -8.4574813843, -2.9389066696]],\n",
      "\n",
      "        [[-0.4088548422,  1.1165663004,  0.2779664397,  ...,\n",
      "           1.2311091423,  5.5027370453,  2.7962324619]],\n",
      "\n",
      "        [[-2.9439191818, -2.1799809933, -0.3025638759,  ...,\n",
      "           1.5722029209, -1.6050769091, -5.3800115585]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Similarities\n",
      "tensor([[ 0.0509518050],\n",
      "        [-0.0036834651],\n",
      "        [-0.0271014031],\n",
      "        [-0.0133023197],\n",
      "        [-0.0284152422],\n",
      "        [ 0.0232533831],\n",
      "        [-0.0137183340],\n",
      "        [ 0.0186120700],\n",
      "        [ 0.0130030001],\n",
      "        [-0.0065820492],\n",
      "        [ 0.0332949199],\n",
      "        [ 0.0076509402],\n",
      "        [-0.0420302823],\n",
      "        [-0.0632281080],\n",
      "        [-0.0435007587],\n",
      "        [ 0.0285379384],\n",
      "        [ 0.0427818298],\n",
      "        [ 0.0048807422],\n",
      "        [-0.0212702043],\n",
      "        [-0.0244439747],\n",
      "        [ 0.0797368288],\n",
      "        [-0.0160176754],\n",
      "        [ 0.0314475521],\n",
      "        [ 0.0082555059],\n",
      "        [-0.0284099095],\n",
      "        [ 0.0391580015],\n",
      "        [-0.0028492918],\n",
      "        [ 0.0077468990],\n",
      "        [ 0.0279765055],\n",
      "        [ 0.0174491610],\n",
      "        [-0.0558925532],\n",
      "        [-0.0049739028],\n",
      "        [-0.0129930824],\n",
      "        [ 0.0089894840],\n",
      "        [ 0.0224872362],\n",
      "        [ 0.0028382652],\n",
      "        [ 0.0075487052],\n",
      "        [-0.0339910910],\n",
      "        [ 0.0072584008],\n",
      "        [-0.0037173422],\n",
      "        [ 0.0148731703],\n",
      "        [ 0.0814164281],\n",
      "        [ 0.0418928564],\n",
      "        [ 0.0360248163],\n",
      "        [-0.0322922431],\n",
      "        [ 0.0035602038],\n",
      "        [ 0.0134807723],\n",
      "        [ 0.0116694821],\n",
      "        [ 0.0215012860],\n",
      "        [ 0.0126640340],\n",
      "        [-0.0264885984],\n",
      "        [-0.0120642912],\n",
      "        [-0.0099798981],\n",
      "        [-0.0681002662],\n",
      "        [-0.0370535888],\n",
      "        [-0.0736160055],\n",
      "        [ 0.0702925026],\n",
      "        [-0.0485503040],\n",
      "        [-0.0018044645],\n",
      "        [-0.0038489925],\n",
      "        [ 0.0139966439],\n",
      "        [ 0.0064258790],\n",
      "        [ 0.0508244783],\n",
      "        [ 0.0187130328]], grad_fn=<DivBackward0>)\n",
      "Logits\n",
      "tensor([[10.3868970871],\n",
      "        [ 9.8573856354],\n",
      "        [ 9.6304244995],\n",
      "        [ 9.7641620636],\n",
      "        [ 9.6176910400],\n",
      "        [10.1184511185],\n",
      "        [ 9.7601299286],\n",
      "        [10.0734682083],\n",
      "        [10.0191068649],\n",
      "        [ 9.8292932510],\n",
      "        [10.2157707214],\n",
      "        [ 9.9672355652],\n",
      "        [ 9.4857368469],\n",
      "        [ 9.2802925110],\n",
      "        [ 9.4714851379],\n",
      "        [10.1696672440],\n",
      "        [10.3077163696],\n",
      "        [ 9.9403877258],\n",
      "        [ 9.6869382858],\n",
      "        [ 9.6561794281],\n",
      "        [10.6658744812],\n",
      "        [ 9.7378454208],\n",
      "        [10.1978664398],\n",
      "        [ 9.9730949402],\n",
      "        [ 9.6177425385],\n",
      "        [10.2725944519],\n",
      "        [ 9.8654699326],\n",
      "        [ 9.9681653976],\n",
      "        [10.1642265320],\n",
      "        [10.0621976852],\n",
      "        [ 9.3513870239],\n",
      "        [ 9.8448781967],\n",
      "        [ 9.7671585083],\n",
      "        [ 9.9802083969],\n",
      "        [10.1110258102],\n",
      "        [ 9.9205923080],\n",
      "        [ 9.9662446976],\n",
      "        [ 9.5636510849],\n",
      "        [ 9.9634313583],\n",
      "        [ 9.8570566177],\n",
      "        [10.0372314453],\n",
      "        [10.6821537018],\n",
      "        [10.2990999222],\n",
      "        [10.2422285080],\n",
      "        [ 9.5801153183],\n",
      "        [ 9.9275894165],\n",
      "        [10.0237369537],\n",
      "        [10.0061826706],\n",
      "        [10.1014699936],\n",
      "        [10.0158214569],\n",
      "        [ 9.6363630295],\n",
      "        [ 9.7761602402],\n",
      "        [ 9.7963619232],\n",
      "        [ 9.2330732346],\n",
      "        [ 9.5339698792],\n",
      "        [ 9.1796159744],\n",
      "        [10.5743427277],\n",
      "        [ 9.4225463867],\n",
      "        [ 9.8755960464],\n",
      "        [ 9.8557806015],\n",
      "        [10.0287370682],\n",
      "        [ 9.9553623199],\n",
      "        [10.3856630325],\n",
      "        [10.0744466782]], grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.9999791384],\n",
       "        [0.9999576807],\n",
       "        [0.9999443293],\n",
       "        [0.9999525547],\n",
       "        [0.9999434948],\n",
       "        [0.9999697208],\n",
       "        [0.9999523163],\n",
       "        [0.9999678135],\n",
       "        [0.9999654293],\n",
       "        [0.9999561310],\n",
       "        [0.9999734163],\n",
       "        [0.9999630451],\n",
       "        [0.9999340773],\n",
       "        [0.9999167919],\n",
       "        [0.9999330044],\n",
       "        [0.9999717474],\n",
       "        [0.9999766350],\n",
       "        [0.9999618530],\n",
       "        [0.9999479055],\n",
       "        [0.9999459982],\n",
       "        [0.9999866486],\n",
       "        [0.9999510050],\n",
       "        [0.9999728203],\n",
       "        [0.9999634027],\n",
       "        [0.9999434948],\n",
       "        [0.9999754429],\n",
       "        [0.9999580383],\n",
       "        [0.9999631643],\n",
       "        [0.9999715090],\n",
       "        [0.9999673367],\n",
       "        [0.9999231100],\n",
       "        [0.9999569654],\n",
       "        [0.9999526739],\n",
       "        [0.9999637604],\n",
       "        [0.9999693632],\n",
       "        [0.9999608994],\n",
       "        [0.9999630451],\n",
       "        [0.9999397993],\n",
       "        [0.9999629259],\n",
       "        [0.9999576807],\n",
       "        [0.9999662638],\n",
       "        [0.9999870062],\n",
       "        [0.9999763966],\n",
       "        [0.9999743700],\n",
       "        [0.9999408722],\n",
       "        [0.9999612570],\n",
       "        [0.9999656677],\n",
       "        [0.9999649525],\n",
       "        [0.9999690056],\n",
       "        [0.9999653101],\n",
       "        [0.9999446869],\n",
       "        [0.9999532700],\n",
       "        [0.9999543428],\n",
       "        [0.9999122620],\n",
       "        [0.9999376535],\n",
       "        [0.9999068975],\n",
       "        [0.9999845028],\n",
       "        [0.9999291897],\n",
       "        [0.9999586344],\n",
       "        [0.9999575615],\n",
       "        [0.9999659061],\n",
       "        [0.9999625683],\n",
       "        [0.9999791384],\n",
       "        [0.9999678135]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(precision=10)\n",
    "model(args) + 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 17.964901\n",
      "Loss: 12.851678\n",
      "Loss: 3.3438678\n",
      "Loss: 0.95996743\n",
      "Loss: 0.34611475\n",
      "Loss: 0.15378752\n",
      "Loss: 0.08027104\n",
      "Loss: 0.047051396\n",
      "Loss: 0.029958203\n",
      "Loss: 0.020270685\n",
      "Loss: 0.014373403\n",
      "Loss: 0.010583589\n",
      "Loss: 0.00804223\n",
      "Loss: 0.0062779696\n",
      "Loss: 0.0050170803\n",
      "Loss: 0.0040932065\n",
      "Loss: 0.0034014245\n",
      "Loss: 0.002873423\n",
      "Loss: 0.0024635247\n",
      "Loss: 0.002140436\n",
      "Loss: 0.0018822715\n",
      "Loss: 0.0016734287\n",
      "Loss: 0.0015025898\n",
      "Loss: 0.0013614183\n",
      "Loss: 0.0012436841\n",
      "Loss: 0.0011446679\n",
      "Loss: 0.0010607538\n",
      "Loss: 0.0009891376\n",
      "Loss: 0.0009276207\n",
      "Loss: 0.00087446626\n",
      "Loss: 0.0008282855\n",
      "Loss: 0.0007879608\n",
      "Loss: 0.0007525835\n",
      "Loss: 0.000721412\n",
      "Loss: 0.00069383346\n",
      "Loss: 0.0006693418\n",
      "Loss: 0.00064751366\n",
      "Loss: 0.00062799256\n",
      "Loss: 0.0006104789\n",
      "Loss: 0.0005947185\n",
      "Loss: 0.0005804937\n",
      "Loss: 0.00056761847\n",
      "Loss: 0.0005559328\n",
      "Loss: 0.00054529915\n",
      "Loss: 0.0005355967\n",
      "Loss: 0.0005267216\n",
      "Loss: 0.000518582\n",
      "Loss: 0.000511099\n",
      "Loss: 0.00050420145\n",
      "Loss: 0.0004978282\n",
      "Loss: 0.0004919249\n",
      "Loss: 0.00048644276\n",
      "Loss: 0.00048133967\n",
      "Loss: 0.000476577\n",
      "Loss: 0.0004721214\n",
      "Loss: 0.0004679416\n",
      "Loss: 0.00046401203\n",
      "Loss: 0.00046030703\n",
      "Loss: 0.00045680618\n",
      "Loss: 0.00045348995\n",
      "Loss: 0.00045034068\n",
      "Loss: 0.0004473425\n",
      "Loss: 0.0004444825\n",
      "Loss: 0.0004417462\n",
      "Loss: 0.00043912392\n",
      "Loss: 0.00043660446\n",
      "Loss: 0.00043417883\n",
      "Loss: 0.00043183874\n",
      "Loss: 0.00042957652\n",
      "Loss: 0.00042738594\n",
      "Loss: 0.00042526037\n",
      "Loss: 0.00042319435\n",
      "Loss: 0.00042118324\n",
      "Loss: 0.00041922182\n",
      "Loss: 0.00041730632\n",
      "Loss: 0.00041543331\n",
      "Loss: 0.00041359884\n",
      "Loss: 0.00041180075\n",
      "Loss: 0.00041003528\n",
      "Loss: 0.00040830093\n",
      "Loss: 0.0004065953\n",
      "Loss: 0.00040491566\n",
      "Loss: 0.00040326075\n",
      "Loss: 0.00040162896\n",
      "Loss: 0.0004000188\n",
      "Loss: 0.00039842902\n",
      "Loss: 0.0003968578\n",
      "Loss: 0.00039530447\n",
      "Loss: 0.00039376804\n",
      "Loss: 0.00039224734\n",
      "Loss: 0.00039074177\n",
      "Loss: 0.00038925072\n",
      "Loss: 0.00038777306\n",
      "Loss: 0.0003863087\n",
      "Loss: 0.00038485666\n",
      "Loss: 0.0003834169\n",
      "Loss: 0.00038198868\n",
      "Loss: 0.00038057152\n",
      "Loss: 0.00037916508\n",
      "Loss: 0.00037776897\n",
      "Loss: 0.0003763831\n",
      "Loss: 0.0003750067\n",
      "Loss: 0.00037364027\n",
      "Loss: 0.00037228302\n",
      "Loss: 0.00037093495\n",
      "Loss: 0.00036959597\n",
      "Loss: 0.00036826567\n",
      "Loss: 0.0003669441\n",
      "Loss: 0.0003656307\n",
      "Loss: 0.00036432542\n",
      "Loss: 0.00036302852\n",
      "Loss: 0.00036173972\n",
      "Loss: 0.00036045891\n",
      "Loss: 0.0003591857\n",
      "Loss: 0.00035792033\n",
      "Loss: 0.0003566625\n",
      "Loss: 0.0003554125\n",
      "Loss: 0.00035416964\n",
      "Loss: 0.00035293438\n",
      "Loss: 0.00035170652\n",
      "Loss: 0.00035048585\n",
      "Loss: 0.00034927242\n",
      "Loss: 0.00034806656\n",
      "Loss: 0.00034686754\n",
      "Loss: 0.00034567542\n",
      "Loss: 0.00034449043\n",
      "Loss: 0.00034331225\n",
      "Loss: 0.00034214117\n",
      "Loss: 0.00034097672\n",
      "Loss: 0.00033981894\n",
      "Loss: 0.0003386682\n",
      "Loss: 0.0003375242\n",
      "Loss: 0.00033638685\n",
      "Loss: 0.00033525613\n",
      "Loss: 0.00033413165\n",
      "Loss: 0.00033301403\n",
      "Loss: 0.00033190273\n",
      "Loss: 0.00033079792\n",
      "Loss: 0.0003296995\n",
      "Loss: 0.00032860742\n",
      "Loss: 0.00032752188\n",
      "Loss: 0.0003264422\n",
      "Loss: 0.0003253692\n",
      "Loss: 0.0003243021\n",
      "Loss: 0.0003232414\n",
      "Loss: 0.00032218668\n",
      "Loss: 0.00032113813\n",
      "Loss: 0.00032009554\n",
      "Loss: 0.00031905912\n",
      "Loss: 0.0003180286\n",
      "Loss: 0.00031700407\n",
      "Loss: 0.00031598538\n",
      "Loss: 0.00031497274\n",
      "Loss: 0.00031396587\n",
      "Loss: 0.0003129646\n",
      "Loss: 0.00031196917\n",
      "Loss: 0.00031097973\n",
      "Loss: 0.00030999558\n",
      "Loss: 0.00030901725\n",
      "Loss: 0.0003080445\n",
      "Loss: 0.0003070773\n",
      "Loss: 0.0003061153\n",
      "Loss: 0.000305159\n",
      "Loss: 0.00030420782\n",
      "Loss: 0.0003032624\n",
      "Loss: 0.00030232215\n",
      "Loss: 0.00030138742\n",
      "Loss: 0.00030045782\n",
      "Loss: 0.0002995335\n",
      "Loss: 0.00029861453\n",
      "Loss: 0.00029770055\n",
      "Loss: 0.00029679196\n",
      "Loss: 0.0002958885\n",
      "Loss: 0.00029498994\n",
      "Loss: 0.00029409648\n",
      "Loss: 0.00029320817\n",
      "Loss: 0.00029232475\n",
      "Loss: 0.0002914464\n",
      "Loss: 0.00029057276\n",
      "Loss: 0.00028970407\n",
      "Loss: 0.00028884047\n",
      "Loss: 0.00028798133\n",
      "Loss: 0.00028712713\n",
      "Loss: 0.00028627765\n",
      "Loss: 0.00028543305\n",
      "Loss: 0.00028459282\n",
      "Loss: 0.0002837574\n",
      "Loss: 0.0002829265\n",
      "Loss: 0.00028210017\n",
      "Loss: 0.00028127854\n",
      "Loss: 0.00028046157\n",
      "Loss: 0.0002796491\n",
      "Loss: 0.00027884112\n",
      "Loss: 0.00027803745\n",
      "Loss: 0.0002772382\n",
      "Loss: 0.00027644332\n",
      "Loss: 0.00027565274\n",
      "Loss: 0.00027486635\n",
      "Loss: 0.0002740844\n",
      "Loss: 0.00027330653\n",
      "Loss: 0.00027253298\n",
      "Loss: 0.00027176342\n",
      "Loss: 0.0002709981\n",
      "Loss: 0.00027023684\n",
      "Loss: 0.00026947985\n",
      "Loss: 0.00026872667\n",
      "Loss: 0.00026797748\n",
      "Loss: 0.00026723245\n",
      "Loss: 0.00026649126\n",
      "Loss: 0.000265754\n",
      "Loss: 0.0002650208\n",
      "Loss: 0.00026429127\n",
      "Loss: 0.00026356571\n",
      "Loss: 0.00026284382\n",
      "Loss: 0.00026212598\n",
      "Loss: 0.000261412\n",
      "Loss: 0.00026070164\n",
      "Loss: 0.0002599952\n",
      "Loss: 0.00025929237\n",
      "Loss: 0.00025859303\n",
      "Loss: 0.0002578975\n",
      "Loss: 0.0002572055\n",
      "Loss: 0.00025651706\n",
      "Loss: 0.00025583227\n",
      "Loss: 0.00025515095\n",
      "Loss: 0.0002544731\n",
      "Loss: 0.00025379867\n",
      "Loss: 0.0002531278\n",
      "Loss: 0.00025246028\n",
      "Loss: 0.00025179624\n",
      "Loss: 0.00025113547\n",
      "Loss: 0.00025047836\n",
      "Loss: 0.00024982466\n",
      "Loss: 0.00024917407\n",
      "Loss: 0.00024852692\n",
      "Loss: 0.0002478829\n",
      "Loss: 0.00024724225\n",
      "Loss: 0.00024660476\n",
      "Loss: 0.0002459704\n",
      "Loss: 0.00024533915\n",
      "Loss: 0.00024471123\n",
      "Loss: 0.00024408635\n",
      "Loss: 0.0002434645\n",
      "Loss: 0.00024284577\n",
      "Loss: 0.00024223007\n",
      "Loss: 0.00024161773\n",
      "Loss: 0.00024100828\n",
      "Loss: 0.00024040199\n",
      "Loss: 0.0002397986\n",
      "Loss: 0.0002391981\n",
      "Loss: 0.00023860046\n",
      "Loss: 0.0002380058\n",
      "Loss: 0.00023741407\n",
      "Loss: 0.00023682512\n",
      "Loss: 0.00023623915\n",
      "Loss: 0.00023565591\n",
      "Loss: 0.00023507554\n",
      "Loss: 0.0002344979\n",
      "Loss: 0.00023392325\n",
      "Loss: 0.00023335137\n",
      "Loss: 0.00023278223\n",
      "Loss: 0.00023221572\n",
      "Loss: 0.00023165203\n",
      "Loss: 0.00023109093\n",
      "Loss: 0.00023053255\n",
      "Loss: 0.00022997678\n",
      "Loss: 0.00022942353\n",
      "Loss: 0.000228873\n",
      "Loss: 0.00022832512\n",
      "Loss: 0.00022777963\n",
      "Loss: 0.00022723703\n",
      "Loss: 0.00022669688\n",
      "Loss: 0.00022615932\n",
      "Loss: 0.0002256242\n",
      "Loss: 0.00022509164\n",
      "Loss: 0.0002245614\n",
      "Loss: 0.00022403369\n",
      "Loss: 0.00022350838\n",
      "Loss: 0.00022298555\n",
      "Loss: 0.00022246502\n",
      "Loss: 0.00022194699\n",
      "Loss: 0.00022143121\n",
      "Loss: 0.00022091808\n",
      "Loss: 0.00022040724\n",
      "Loss: 0.00021989875\n",
      "Loss: 0.0002193925\n",
      "Loss: 0.00021888854\n",
      "Loss: 0.00021838695\n",
      "Loss: 0.00021788756\n",
      "Loss: 0.00021739032\n",
      "Loss: 0.00021689542\n",
      "Loss: 0.00021640268\n",
      "Loss: 0.00021591214\n",
      "Loss: 0.00021542404\n",
      "Loss: 0.00021493809\n",
      "Loss: 0.00021445437\n",
      "Loss: 0.00021397251\n",
      "Loss: 0.00021349307\n",
      "Loss: 0.0002130156\n",
      "Loss: 0.00021254034\n",
      "Loss: 0.00021206703\n",
      "Loss: 0.00021159588\n",
      "Loss: 0.0002111267\n",
      "Loss: 0.00021065968\n",
      "Loss: 0.0002101946\n",
      "Loss: 0.00020973172\n",
      "Loss: 0.00020927098\n",
      "Loss: 0.00020881211\n",
      "Loss: 0.00020835525\n",
      "Loss: 0.00020790039\n",
      "Loss: 0.00020744745\n",
      "Loss: 0.00020699647\n",
      "Loss: 0.00020654745\n",
      "Loss: 0.00020610023\n",
      "Loss: 0.00020565497\n",
      "Loss: 0.00020521166\n",
      "Loss: 0.00020477036\n",
      "Loss: 0.00020433098\n",
      "Loss: 0.00020389335\n",
      "Loss: 0.00020345768\n",
      "Loss: 0.0002030237\n",
      "Loss: 0.00020259173\n",
      "Loss: 0.00020216152\n",
      "Loss: 0.00020173307\n",
      "Loss: 0.00020130633\n",
      "Loss: 0.00020088148\n",
      "Loss: 0.00020045838\n",
      "Loss: 0.00020003713\n",
      "Loss: 0.00019961772\n",
      "Loss: 0.00019919997\n",
      "Loss: 0.000198784\n",
      "Loss: 0.00019836976\n",
      "Loss: 0.00019795724\n",
      "Loss: 0.0001975463\n",
      "Loss: 0.0001971371\n",
      "Loss: 0.00019672955\n",
      "Loss: 0.00019632366\n",
      "Loss: 0.00019591943\n",
      "Loss: 0.00019551703\n",
      "Loss: 0.00019511621\n",
      "Loss: 0.00019471704\n",
      "Loss: 0.00019431955\n",
      "Loss: 0.00019392354\n",
      "Loss: 0.0001935292\n",
      "Loss: 0.00019313638\n",
      "Loss: 0.00019274512\n",
      "Loss: 0.0001923555\n",
      "Loss: 0.00019196738\n",
      "Loss: 0.00019158082\n",
      "Loss: 0.00019119575\n",
      "Loss: 0.00019081241\n",
      "Loss: 0.00019043058\n",
      "Loss: 0.00019005027\n",
      "Loss: 0.00018967147\n",
      "Loss: 0.00018929414\n",
      "Loss: 0.00018891823\n",
      "Loss: 0.00018854385\n",
      "Loss: 0.00018817084\n",
      "Loss: 0.00018779938\n",
      "Loss: 0.0001874294\n",
      "Loss: 0.00018706075\n",
      "Loss: 0.0001866938\n",
      "Loss: 0.00018632814\n",
      "Loss: 0.00018596396\n",
      "Loss: 0.0001856012\n",
      "Loss: 0.0001852398\n",
      "Loss: 0.00018487984\n",
      "Loss: 0.00018452115\n",
      "Loss: 0.00018416392\n",
      "Loss: 0.00018380802\n",
      "Loss: 0.0001834534\n",
      "Loss: 0.00018310029\n",
      "Loss: 0.00018274841\n",
      "Loss: 0.00018239807\n",
      "Loss: 0.000182049\n",
      "Loss: 0.00018170127\n",
      "Loss: 0.00018135482\n",
      "Loss: 0.00018100967\n",
      "Loss: 0.00018066585\n",
      "Loss: 0.0001803233\n",
      "Loss: 0.00017998197\n",
      "Loss: 0.00017964205\n",
      "Loss: 0.00017930326\n",
      "Loss: 0.00017896571\n",
      "Loss: 0.00017862968\n",
      "Loss: 0.00017829478\n",
      "Loss: 0.00017796122\n",
      "Loss: 0.00017762887\n",
      "Loss: 0.0001772977\n",
      "Loss: 0.00017696772\n",
      "Loss: 0.00017663899\n",
      "Loss: 0.00017631143\n",
      "Loss: 0.00017598506\n",
      "Loss: 0.0001756599\n",
      "Loss: 0.00017533591\n",
      "Loss: 0.00017501309\n",
      "Loss: 0.00017469157\n",
      "Loss: 0.00017437125\n",
      "Loss: 0.00017405205\n",
      "Loss: 0.00017373406\n",
      "Loss: 0.00017341717\n",
      "Loss: 0.00017310139\n",
      "Loss: 0.00017278672\n",
      "Loss: 0.00017247327\n",
      "Loss: 0.0001721608\n",
      "Loss: 0.00017184958\n",
      "Loss: 0.0001715394\n",
      "Loss: 0.00017123025\n",
      "Loss: 0.00017092246\n",
      "Loss: 0.0001706157\n",
      "Loss: 0.00017030998\n",
      "Loss: 0.00017000541\n",
      "Loss: 0.00016970183\n",
      "Loss: 0.00016939937\n",
      "Loss: 0.000169098\n",
      "Loss: 0.00016879759\n",
      "Loss: 0.00016849824\n",
      "Loss: 0.00016819994\n",
      "Loss: 0.00016790269\n",
      "Loss: 0.00016760647\n",
      "Loss: 0.00016731145\n",
      "Loss: 0.0001670174\n",
      "Loss: 0.0001667244\n",
      "Loss: 0.00016643238\n",
      "Loss: 0.00016614131\n",
      "Loss: 0.0001658513\n",
      "Loss: 0.00016556232\n",
      "Loss: 0.00016527418\n",
      "Loss: 0.00016498716\n",
      "Loss: 0.0001647011\n",
      "Loss: 0.00016441596\n",
      "Loss: 0.00016413174\n",
      "Loss: 0.00016384854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.00016356644\n",
      "Loss: 0.00016328531\n",
      "Loss: 0.00016300507\n",
      "Loss: 0.00016272582\n",
      "Loss: 0.00016244744\n",
      "Loss: 0.00016217011\n",
      "Loss: 0.0001618936\n",
      "Loss: 0.00016161801\n",
      "Loss: 0.00016134337\n",
      "Loss: 0.00016106965\n",
      "Loss: 0.00016079679\n",
      "Loss: 0.00016052487\n",
      "Loss: 0.00016025393\n",
      "Loss: 0.00015998397\n",
      "Loss: 0.00015971485\n",
      "Loss: 0.00015944662\n",
      "Loss: 0.00015917922\n",
      "Loss: 0.00015891276\n",
      "Loss: 0.00015864713\n",
      "Loss: 0.00015838242\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-86fa3c98454d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprobas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-49cc0779f3f4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_dict)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mprojections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0msimilarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    probas = model(args)\n",
    "    loss = probas.sum()\n",
    "    print('Loss:', loss.detach().numpy())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во всяком случае, на текущий момент возможно переобучить модель под нужные значения\n",
    "_________________\n",
    "\n",
    "**Цикл обучения модели**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы обучить модель, надо сделать следующие шаги:\n",
    "\n",
    "* Добавить пары корректных гипонимов-гиперонимов\n",
    "* Для каждого положительного добавить несколько отрицательных (пока просто случайные слова)\n",
    "* Добавить вектор правильных ответов\n",
    "\n",
    "Сделаем из этого `torch.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HypernymQueriesDataset(Dataset):\n",
    "    def load_train_items(self, thesaurus, vocab, train_keys):\n",
    "        self.train_items = []\n",
    "        for hyponym in tqdm(train_keys):\n",
    "            hypernyms = thesaurus.hypernyms_dict[hyponym]\n",
    "            for hypernym in hypernyms:\n",
    "                self.train_items.append([hyponym, hypernym, True])\n",
    "            \n",
    "            negative_examples = np.random.choice(list(vocab.keys()), size=self.n_negative * len(hypernyms))\n",
    "            for negative in negative_examples:\n",
    "                self.train_items.append([hyponym, negative, False])\n",
    "                \n",
    "            if self.max_pairs is not None and len(self.train_items) > self.max_pairs:\n",
    "                break\n",
    "    \n",
    "    def __init__(self, thesaurus, vocab, train_keys, n_negative=3, max_pairs=None):\n",
    "        self.n_negative = n_negative\n",
    "        self.thesaurus = thesaurus\n",
    "        self.vocab = vocab\n",
    "        self.max_pairs = max_pairs\n",
    "        \n",
    "        self.load_train_items(thesaurus, vocab, train_keys)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_items)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        hyponym, hypernym, label = self.train_items[idx]\n",
    "        return (self.vocab[hyponym], self.vocab[hypernym], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_keys') as train_file:\n",
    "    train_keys = train_file.readlines()\n",
    "train_keys = [key[:-1] if key[-1] == '\\n' else key for key in train_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4526336f908d4a6687693e337507ce7f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = HypernymQueriesDataset(thesaurus, vocab_embeddings, train_keys, max_pairs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим ещё тестовый датасет, чтобы проверять качество на нём во время обучения. Пока будем смотреть только на функцию ошибки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_keys') as test_file:\n",
    "    test_keys = test_file.readlines()\n",
    "test_keys = [key[:-1] if key[-1] == '\\n' else key for key in test_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159894dc22cf4f1ab84fc932433185db"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = HypernymQueriesDataset(thesaurus, vocab_embeddings, test_keys, max_pairs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CRIMModel(n_matrices=1)\n",
    "optimizer = Adam(model.parameters(), lr=3e-3)\n",
    "n_epochs = 1\n",
    "batch_size = 64\n",
    "plot_frequency = 50\n",
    "val_freq = 1000\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=2048, shuffle=True)\n",
    "for b in test_loader:\n",
    "    test_batch = b\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loss_on_test(model):\n",
    "    model.eval()\n",
    "    hyponyms, candidates, labels = test_batch\n",
    "    model_batch = {\n",
    "        'batch': hyponyms,\n",
    "        'candidate': candidates\n",
    "    }\n",
    "    labels = labels.float()\n",
    "    logits = model(model_batch).squeeze()\n",
    "    loss = loss_fn(logits, labels)\n",
    "    return loss.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "test_loss_history = []\n",
    "matrix_norms = []\n",
    "weight_norms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Started training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a8475291984fb187f43096adf1cd89"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 300])\n",
      "Batch\n",
      "tensor([[[ 0.0261],\n",
      "         [ 0.0979],\n",
      "         [-0.0401],\n",
      "         ...,\n",
      "         [-0.0251],\n",
      "         [-0.0500],\n",
      "         [-0.0133]],\n",
      "\n",
      "        [[-0.0273],\n",
      "         [-0.0372],\n",
      "         [-0.0413],\n",
      "         ...,\n",
      "         [-0.0141],\n",
      "         [ 0.0174],\n",
      "         [-0.0078]],\n",
      "\n",
      "        [[ 0.0261],\n",
      "         [ 0.0979],\n",
      "         [-0.0401],\n",
      "         ...,\n",
      "         [-0.0251],\n",
      "         [-0.0500],\n",
      "         [-0.0133]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0261],\n",
      "         [ 0.0979],\n",
      "         [-0.0401],\n",
      "         ...,\n",
      "         [-0.0251],\n",
      "         [-0.0500],\n",
      "         [-0.0133]],\n",
      "\n",
      "        [[ 0.0261],\n",
      "         [ 0.0979],\n",
      "         [-0.0401],\n",
      "         ...,\n",
      "         [-0.0251],\n",
      "         [-0.0500],\n",
      "         [-0.0133]],\n",
      "\n",
      "        [[ 0.0229],\n",
      "         [-0.0117],\n",
      "         [-0.0763],\n",
      "         ...,\n",
      "         [ 0.0146],\n",
      "         [ 0.0227],\n",
      "         [-0.0775]]])\n",
      "Matrices\n",
      "Parameter containing:\n",
      "tensor([[[[ 1.0217e+00, -1.3330e-02,  3.5254e-03,  ..., -4.1922e-04,\n",
      "            2.1876e-02,  9.6231e-03],\n",
      "          [ 4.3689e-03,  1.0015e+00,  1.5598e-02,  ...,  9.1161e-03,\n",
      "           -7.5588e-03,  8.3684e-03],\n",
      "          [-1.2749e-02,  1.3711e-02,  1.0054e+00,  ..., -3.2704e-03,\n",
      "           -1.1530e-02, -1.8196e-02],\n",
      "          ...,\n",
      "          [-1.3683e-02, -2.3916e-02,  1.9332e-02,  ...,  1.0238e+00,\n",
      "            2.2175e-03,  3.3672e-03],\n",
      "          [ 1.2501e-02, -1.3812e-02, -3.0965e-02,  ...,  2.8624e-02,\n",
      "            9.7698e-01, -1.7524e-02],\n",
      "          [-1.4824e-02, -1.5939e-02,  7.4597e-03,  ...,  9.5928e-03,\n",
      "            1.4972e-05,  1.0047e+00]]]], requires_grad=True)\n",
      "Projections\n",
      "tensor([[[-0.0003,  0.0355,  0.0252,  ..., -0.0179,  0.0496, -0.0627]],\n",
      "\n",
      "        [[-0.0862, -0.1548,  0.0853,  ...,  0.0531,  0.1429, -0.0173]],\n",
      "\n",
      "        [[-0.0003,  0.0355,  0.0252,  ..., -0.0179,  0.0496, -0.0627]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0003,  0.0355,  0.0252,  ..., -0.0179,  0.0496, -0.0627]],\n",
      "\n",
      "        [[-0.0003,  0.0355,  0.0252,  ..., -0.0179,  0.0496, -0.0627]],\n",
      "\n",
      "        [[-0.0585, -0.0900, -0.0025,  ..., -0.0021,  0.1272, -0.1010]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Similarities\n",
      "tensor([[-0.2885],\n",
      "        [-0.2333],\n",
      "        [-0.1966],\n",
      "        [-0.2785],\n",
      "        [-0.1912],\n",
      "        [-0.0141],\n",
      "        [-0.3840],\n",
      "        [-0.3077],\n",
      "        [-0.3537],\n",
      "        [-0.2790],\n",
      "        [-0.3444],\n",
      "        [-0.2062],\n",
      "        [-0.3053],\n",
      "        [-0.3085],\n",
      "        [-0.0423],\n",
      "        [-0.4121],\n",
      "        [-0.2389],\n",
      "        [-0.4146],\n",
      "        [-0.2301],\n",
      "        [-0.2616],\n",
      "        [-0.3289],\n",
      "        [-0.3873],\n",
      "        [-0.2265],\n",
      "        [-0.3668],\n",
      "        [-0.3871],\n",
      "        [-0.3349],\n",
      "        [-0.2180],\n",
      "        [-0.3389],\n",
      "        [-0.0342],\n",
      "        [-0.2740],\n",
      "        [-0.3662],\n",
      "        [-0.2803],\n",
      "        [-0.0642],\n",
      "        [-0.1877],\n",
      "        [ 0.0767],\n",
      "        [-0.3578],\n",
      "        [-0.1905],\n",
      "        [-0.1396],\n",
      "        [-0.0411],\n",
      "        [-0.3398],\n",
      "        [-0.1253],\n",
      "        [-0.0155],\n",
      "        [-0.2017],\n",
      "        [-0.2541],\n",
      "        [-0.3639],\n",
      "        [-0.3108],\n",
      "        [-0.1297],\n",
      "        [-0.2024],\n",
      "        [ 0.0111],\n",
      "        [-0.1354],\n",
      "        [-0.1939],\n",
      "        [-0.1309],\n",
      "        [-0.3056],\n",
      "        [-0.3515],\n",
      "        [-0.1767],\n",
      "        [-0.3902],\n",
      "        [-0.2176],\n",
      "        [-0.2319],\n",
      "        [-0.2992],\n",
      "        [-0.3825],\n",
      "        [ 0.0528],\n",
      "        [-0.4867],\n",
      "        [-0.1864],\n",
      "        [-0.2312]], grad_fn=<DivBackward0>)\n",
      "Logits\n",
      "tensor([[0.6596],\n",
      "        [0.6682],\n",
      "        [0.6740],\n",
      "        [0.6611],\n",
      "        [0.6748],\n",
      "        [0.7025],\n",
      "        [0.6446],\n",
      "        [0.6566],\n",
      "        [0.6494],\n",
      "        [0.6611],\n",
      "        [0.6508],\n",
      "        [0.6725],\n",
      "        [0.6569],\n",
      "        [0.6564],\n",
      "        [0.6981],\n",
      "        [0.6402],\n",
      "        [0.6673],\n",
      "        [0.6398],\n",
      "        [0.6687],\n",
      "        [0.6638],\n",
      "        [0.6532],\n",
      "        [0.6441],\n",
      "        [0.6693],\n",
      "        [0.6473],\n",
      "        [0.6441],\n",
      "        [0.6523],\n",
      "        [0.6706],\n",
      "        [0.6517],\n",
      "        [0.6994],\n",
      "        [0.6618],\n",
      "        [0.6474],\n",
      "        [0.6608],\n",
      "        [0.6947],\n",
      "        [0.6753],\n",
      "        [0.7167],\n",
      "        [0.6487],\n",
      "        [0.6749],\n",
      "        [0.6829],\n",
      "        [0.6983],\n",
      "        [0.6515],\n",
      "        [0.6851],\n",
      "        [0.7023],\n",
      "        [0.6732],\n",
      "        [0.6649],\n",
      "        [0.6478],\n",
      "        [0.6561],\n",
      "        [0.6844],\n",
      "        [0.6730],\n",
      "        [0.7065],\n",
      "        [0.6835],\n",
      "        [0.6744],\n",
      "        [0.6842],\n",
      "        [0.6569],\n",
      "        [0.6497],\n",
      "        [0.6771],\n",
      "        [0.6437],\n",
      "        [0.6707],\n",
      "        [0.6684],\n",
      "        [0.6579],\n",
      "        [0.6448],\n",
      "        [0.7130],\n",
      "        [0.6286],\n",
      "        [0.6755],\n",
      "        [0.6685]], grad_fn=<AddmmBackward>)\n",
      "Matrix grad\n",
      "tensor([[[[ 8.3123e-06,  2.6338e-05,  2.5620e-05,  ..., -9.5426e-06,\n",
      "           -7.9429e-06,  2.1234e-05],\n",
      "          [ 2.4483e-05,  9.1139e-05, -1.8108e-05,  ..., -6.0239e-05,\n",
      "            6.7057e-06,  9.0398e-06],\n",
      "          [ 5.6303e-06,  2.2916e-05,  2.8598e-05,  ..., -3.7258e-06,\n",
      "           -1.3394e-05,  2.1919e-05],\n",
      "          ...,\n",
      "          [ 3.6716e-07, -3.4379e-05, -9.4595e-07,  ...,  3.9233e-05,\n",
      "           -2.6213e-05, -1.3584e-05],\n",
      "          [ 3.0049e-06,  1.3497e-05,  1.9778e-05,  ..., -1.8196e-05,\n",
      "            1.1988e-05,  2.0235e-05],\n",
      "          [-1.5473e-05, -2.3267e-05,  3.6176e-05,  ..., -8.1910e-06,\n",
      "            3.6239e-06,  4.0885e-05]]]])\n",
      "torch.Size([64, 300])\n",
      "Batch\n",
      "tensor([[[-0.0273],\n",
      "         [-0.0372],\n",
      "         [-0.0413],\n",
      "         ...,\n",
      "         [-0.0141],\n",
      "         [ 0.0174],\n",
      "         [-0.0078]],\n",
      "\n",
      "        [[ 0.0261],\n",
      "         [ 0.0979],\n",
      "         [-0.0401],\n",
      "         ...,\n",
      "         [-0.0251],\n",
      "         [-0.0500],\n",
      "         [-0.0133]],\n",
      "\n",
      "        [[ 0.0229],\n",
      "         [-0.0117],\n",
      "         [-0.0763],\n",
      "         ...,\n",
      "         [ 0.0146],\n",
      "         [ 0.0227],\n",
      "         [-0.0775]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0273],\n",
      "         [-0.0372],\n",
      "         [-0.0413],\n",
      "         ...,\n",
      "         [-0.0141],\n",
      "         [ 0.0174],\n",
      "         [-0.0078]],\n",
      "\n",
      "        [[-0.0125],\n",
      "         [ 0.0616],\n",
      "         [ 0.0235],\n",
      "         ...,\n",
      "         [ 0.0319],\n",
      "         [-0.0066],\n",
      "         [-0.0317]],\n",
      "\n",
      "        [[ 0.0229],\n",
      "         [-0.0117],\n",
      "         [-0.0763],\n",
      "         ...,\n",
      "         [ 0.0146],\n",
      "         [ 0.0227],\n",
      "         [-0.0775]]])\n",
      "Matrices\n",
      "Parameter containing:\n",
      "tensor([[[[ 1.0203e+00, -1.3559e-02,  2.6357e-03,  ..., -1.8547e-03,\n",
      "            2.3076e-02,  1.0271e-02],\n",
      "          [ 3.4705e-03,  1.0005e+00,  1.8104e-02,  ...,  9.2869e-03,\n",
      "           -6.4587e-03,  1.0487e-02],\n",
      "          [-1.4233e-02,  1.2114e-02,  1.0033e+00,  ..., -1.7579e-03,\n",
      "           -1.2480e-02, -2.1017e-02],\n",
      "          ...,\n",
      "          [-1.4673e-02, -2.4451e-02,  1.9399e-02,  ...,  1.0243e+00,\n",
      "            1.4208e-03,  4.3416e-03],\n",
      "          [ 1.3533e-02, -1.3214e-02, -3.3318e-02,  ...,  3.0414e-02,\n",
      "            9.7485e-01, -2.0244e-02],\n",
      "          [-1.3916e-02, -1.4780e-02,  5.7859e-03,  ...,  7.8066e-03,\n",
      "            6.9305e-04,  1.0030e+00]]]], requires_grad=True)\n",
      "Projections\n",
      "tensor([[[-0.0772, -0.1665,  0.1065,  ...,  0.0644,  0.1626, -0.0091]],\n",
      "\n",
      "        [[ 0.0025,  0.0201,  0.0349,  ..., -0.0109,  0.0672, -0.0563]],\n",
      "\n",
      "        [[-0.0631, -0.1026,  0.0177,  ..., -0.0054,  0.1465, -0.0850]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0772, -0.1665,  0.1065,  ...,  0.0644,  0.1626, -0.0091]],\n",
      "\n",
      "        [[-0.0807,  0.0015,  0.1152,  ...,  0.0647,  0.0769, -0.0565]],\n",
      "\n",
      "        [[-0.0631, -0.1026,  0.0177,  ..., -0.0054,  0.1465, -0.0850]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Similarities\n",
      "tensor([[-0.3736],\n",
      "        [-0.3223],\n",
      "        [-0.3873],\n",
      "        [-0.3836],\n",
      "        [-0.3543],\n",
      "        [-0.3960],\n",
      "        [-0.0907],\n",
      "        [-0.0528],\n",
      "        [-0.3304],\n",
      "        [-0.4067],\n",
      "        [-0.0677],\n",
      "        [-0.2734],\n",
      "        [-0.2784],\n",
      "        [-0.3792],\n",
      "        [-0.2623],\n",
      "        [-0.3133],\n",
      "        [-0.4173],\n",
      "        [-0.4237],\n",
      "        [-0.0570],\n",
      "        [-0.3844],\n",
      "        [-0.2579],\n",
      "        [-0.2258],\n",
      "        [-0.3358],\n",
      "        [-0.3827],\n",
      "        [-0.1137],\n",
      "        [-0.0395],\n",
      "        [-0.0386],\n",
      "        [-0.3901],\n",
      "        [-0.3525],\n",
      "        [-0.3641],\n",
      "        [-0.4643],\n",
      "        [-0.0367],\n",
      "        [-0.3293],\n",
      "        [-0.4571],\n",
      "        [-0.2403],\n",
      "        [-0.3742],\n",
      "        [-0.4026],\n",
      "        [-0.4483],\n",
      "        [-0.2158],\n",
      "        [-0.1412],\n",
      "        [-0.1069],\n",
      "        [-0.3229],\n",
      "        [-0.3213],\n",
      "        [-0.2930],\n",
      "        [-0.1497],\n",
      "        [-0.3866],\n",
      "        [-0.3734],\n",
      "        [-0.3509],\n",
      "        [-0.4253],\n",
      "        [-0.4539],\n",
      "        [-0.1663],\n",
      "        [-0.3808],\n",
      "        [-0.3560],\n",
      "        [-0.1590],\n",
      "        [-0.4072],\n",
      "        [ 0.0093],\n",
      "        [-0.3615],\n",
      "        [-0.4384],\n",
      "        [-0.1376],\n",
      "        [-0.3389],\n",
      "        [-0.2047],\n",
      "        [-0.3837],\n",
      "        [-0.3063],\n",
      "        [-0.1239]], grad_fn=<DivBackward0>)\n",
      "Logits\n",
      "tensor([[0.6426],\n",
      "        [0.6507],\n",
      "        [0.6404],\n",
      "        [0.6410],\n",
      "        [0.6456],\n",
      "        [0.6390],\n",
      "        [0.6874],\n",
      "        [0.6934],\n",
      "        [0.6494],\n",
      "        [0.6373],\n",
      "        [0.6910],\n",
      "        [0.6584],\n",
      "        [0.6577],\n",
      "        [0.6417],\n",
      "        [0.6602],\n",
      "        [0.6521],\n",
      "        [0.6357],\n",
      "        [0.6347],\n",
      "        [0.6927],\n",
      "        [0.6409],\n",
      "        [0.6609],\n",
      "        [0.6660],\n",
      "        [0.6486],\n",
      "        [0.6411],\n",
      "        [0.6837],\n",
      "        [0.6955],\n",
      "        [0.6956],\n",
      "        [0.6400],\n",
      "        [0.6459],\n",
      "        [0.6441],\n",
      "        [0.6282],\n",
      "        [0.6959],\n",
      "        [0.6496],\n",
      "        [0.6294],\n",
      "        [0.6637],\n",
      "        [0.6425],\n",
      "        [0.6380],\n",
      "        [0.6307],\n",
      "        [0.6676],\n",
      "        [0.6794],\n",
      "        [0.6848],\n",
      "        [0.6506],\n",
      "        [0.6509],\n",
      "        [0.6553],\n",
      "        [0.6780],\n",
      "        [0.6405],\n",
      "        [0.6426],\n",
      "        [0.6462],\n",
      "        [0.6344],\n",
      "        [0.6299],\n",
      "        [0.6754],\n",
      "        [0.6414],\n",
      "        [0.6454],\n",
      "        [0.6766],\n",
      "        [0.6373],\n",
      "        [0.7032],\n",
      "        [0.6445],\n",
      "        [0.6323],\n",
      "        [0.6799],\n",
      "        [0.6481],\n",
      "        [0.6693],\n",
      "        [0.6410],\n",
      "        [0.6532],\n",
      "        [0.6821]], grad_fn=<AddmmBackward>)\n",
      "Matrix grad\n",
      "tensor([[[[ 1.4804e-05,  6.4791e-06, -1.2075e-05,  ...,  2.9599e-07,\n",
      "            4.6170e-06, -1.4934e-05],\n",
      "          [-5.4735e-06, -3.5093e-06,  1.3376e-05,  ...,  1.7317e-05,\n",
      "            6.3436e-06, -9.3725e-06],\n",
      "          [ 6.8454e-06,  1.1135e-05, -1.5334e-05,  ..., -1.5098e-05,\n",
      "            1.4397e-06, -1.0737e-06],\n",
      "          ...,\n",
      "          [ 1.3227e-06,  2.6803e-05, -1.9855e-05,  ..., -7.7862e-06,\n",
      "            4.3241e-06, -1.7750e-05],\n",
      "          [-9.2962e-06, -5.2377e-05,  1.2355e-05,  ...,  1.0754e-06,\n",
      "            1.5768e-05,  1.8315e-05],\n",
      "          [ 3.9250e-08, -1.5805e-05, -8.8032e-06,  ..., -1.5181e-05,\n",
      "            3.9900e-06,  1.2255e-05]]]])\n",
      "torch.Size([64, 300])\n",
      "Batch\n",
      "tensor([[[ 0.0229],\n",
      "         [-0.0117],\n",
      "         [-0.0763],\n",
      "         ...,\n",
      "         [ 0.0146],\n",
      "         [ 0.0227],\n",
      "         [-0.0775]],\n",
      "\n",
      "        [[ 0.0261],\n",
      "         [ 0.0979],\n",
      "         [-0.0401],\n",
      "         ...,\n",
      "         [-0.0251],\n",
      "         [-0.0500],\n",
      "         [-0.0133]],\n",
      "\n",
      "        [[-0.0273],\n",
      "         [-0.0372],\n",
      "         [-0.0413],\n",
      "         ...,\n",
      "         [-0.0141],\n",
      "         [ 0.0174],\n",
      "         [-0.0078]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0125],\n",
      "         [ 0.0616],\n",
      "         [ 0.0235],\n",
      "         ...,\n",
      "         [ 0.0319],\n",
      "         [-0.0066],\n",
      "         [-0.0317]],\n",
      "\n",
      "        [[ 0.0261],\n",
      "         [ 0.0979],\n",
      "         [-0.0401],\n",
      "         ...,\n",
      "         [-0.0251],\n",
      "         [-0.0500],\n",
      "         [-0.0133]],\n",
      "\n",
      "        [[-0.0273],\n",
      "         [-0.0372],\n",
      "         [-0.0413],\n",
      "         ...,\n",
      "         [-0.0141],\n",
      "         [ 0.0174],\n",
      "         [-0.0078]]])\n",
      "Matrices\n",
      "Parameter containing:\n",
      "tensor([[[[ 1.0186e+00, -1.3883e-02,  2.1680e-03,  ..., -3.1143e-03,\n",
      "            2.3966e-02,  1.1172e-02],\n",
      "          [ 2.7828e-03,  9.9966e-01,  2.0060e-02,  ...,  9.1645e-03,\n",
      "           -5.5752e-03,  1.2559e-02],\n",
      "          [-1.5782e-02,  1.0587e-02,  1.0017e+00,  ..., -2.4591e-04,\n",
      "           -1.3340e-02, -2.3455e-02],\n",
      "          ...,\n",
      "          [-1.5577e-02, -2.5203e-02,  1.9974e-02,  ...,  1.0249e+00,\n",
      "            6.4471e-04,  5.9705e-03],\n",
      "          [ 1.4797e-02, -1.2283e-02, -3.5459e-02,  ...,  3.1964e-02,\n",
      "            9.7283e-01, -2.2781e-02],\n",
      "          [-1.3124e-02, -1.3585e-02,  4.4403e-03,  ...,  6.7558e-03,\n",
      "            1.1817e-03,  1.0012e+00]]]], requires_grad=True)\n",
      "Projections\n",
      "tensor([[[-0.0707, -0.1147,  0.0349,  ..., -0.0126,  0.1642, -0.0697]],\n",
      "\n",
      "        [[ 0.0058,  0.0079,  0.0423,  ..., -0.0077,  0.0842, -0.0509]],\n",
      "\n",
      "        [[-0.0673, -0.1760,  0.1249,  ...,  0.0711,  0.1799, -0.0025]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0837, -0.0059,  0.1268,  ...,  0.0648,  0.0896, -0.0498]],\n",
      "\n",
      "        [[ 0.0058,  0.0079,  0.0423,  ..., -0.0077,  0.0842, -0.0509]],\n",
      "\n",
      "        [[-0.0673, -0.1760,  0.1249,  ...,  0.0711,  0.1799, -0.0025]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Similarities\n",
      "tensor([[-0.1186],\n",
      "        [-0.0800],\n",
      "        [-0.4739],\n",
      "        [-0.3958],\n",
      "        [-0.3451],\n",
      "        [-0.4506],\n",
      "        [-0.4294],\n",
      "        [-0.0643],\n",
      "        [-0.3631],\n",
      "        [-0.2898],\n",
      "        [-0.3904],\n",
      "        [-0.2176],\n",
      "        [-0.3757],\n",
      "        [-0.3642],\n",
      "        [-0.3769],\n",
      "        [-0.4464],\n",
      "        [-0.4864],\n",
      "        [-0.3680],\n",
      "        [ 0.0419],\n",
      "        [-0.2230],\n",
      "        [-0.4870],\n",
      "        [-0.1638],\n",
      "        [-0.4139],\n",
      "        [-0.2307],\n",
      "        [-0.3538],\n",
      "        [-0.4269],\n",
      "        [-0.4856],\n",
      "        [-0.0686],\n",
      "        [-0.3358],\n",
      "        [-0.0167],\n",
      "        [-0.4114],\n",
      "        [-0.3651],\n",
      "        [-0.5100],\n",
      "        [-0.1862],\n",
      "        [-0.2974],\n",
      "        [-0.3228],\n",
      "        [-0.4163],\n",
      "        [-0.3839],\n",
      "        [-0.3839],\n",
      "        [-0.2856],\n",
      "        [-0.6011],\n",
      "        [-0.3273],\n",
      "        [-0.5092],\n",
      "        [-0.3727],\n",
      "        [-0.2044],\n",
      "        [-0.5486],\n",
      "        [-0.2717],\n",
      "        [-0.2870],\n",
      "        [-0.3199],\n",
      "        [-0.4077],\n",
      "        [-0.4530],\n",
      "        [-0.3944],\n",
      "        [-0.4679],\n",
      "        [-0.3871],\n",
      "        [-0.3393],\n",
      "        [-0.1341],\n",
      "        [-0.4497],\n",
      "        [-0.3441],\n",
      "        [-0.3881],\n",
      "        [-0.1411],\n",
      "        [-0.4505],\n",
      "        [-0.4236],\n",
      "        [-0.2059],\n",
      "        [-0.2319]], grad_fn=<DivBackward0>)\n",
      "Logits\n",
      "tensor([[0.6797],\n",
      "        [0.6859],\n",
      "        [0.6227],\n",
      "        [0.6352],\n",
      "        [0.6433],\n",
      "        [0.6264],\n",
      "        [0.6298],\n",
      "        [0.6884],\n",
      "        [0.6405],\n",
      "        [0.6522],\n",
      "        [0.6361],\n",
      "        [0.6638],\n",
      "        [0.6384],\n",
      "        [0.6403],\n",
      "        [0.6382],\n",
      "        [0.6271],\n",
      "        [0.6207],\n",
      "        [0.6397],\n",
      "        [0.7055],\n",
      "        [0.6630],\n",
      "        [0.6206],\n",
      "        [0.6725],\n",
      "        [0.6323],\n",
      "        [0.6617],\n",
      "        [0.6420],\n",
      "        [0.6302],\n",
      "        [0.6208],\n",
      "        [0.6877],\n",
      "        [0.6448],\n",
      "        [0.6961],\n",
      "        [0.6327],\n",
      "        [0.6401],\n",
      "        [0.6169],\n",
      "        [0.6689],\n",
      "        [0.6510],\n",
      "        [0.6469],\n",
      "        [0.6319],\n",
      "        [0.6371],\n",
      "        [0.6371],\n",
      "        [0.6529],\n",
      "        [0.6023],\n",
      "        [0.6462],\n",
      "        [0.6170],\n",
      "        [0.6389],\n",
      "        [0.6659],\n",
      "        [0.6107],\n",
      "        [0.6551],\n",
      "        [0.6527],\n",
      "        [0.6474],\n",
      "        [0.6333],\n",
      "        [0.6260],\n",
      "        [0.6354],\n",
      "        [0.6236],\n",
      "        [0.6366],\n",
      "        [0.6443],\n",
      "        [0.6772],\n",
      "        [0.6266],\n",
      "        [0.6435],\n",
      "        [0.6365],\n",
      "        [0.6761],\n",
      "        [0.6264],\n",
      "        [0.6307],\n",
      "        [0.6657],\n",
      "        [0.6615]], grad_fn=<AddmmBackward>)\n",
      "Matrix grad\n",
      "tensor([[[[-2.0600e-06, -4.9673e-05,  7.2787e-06,  ...,  1.2427e-05,\n",
      "            1.7630e-05,  2.5875e-06],\n",
      "          [ 1.3675e-06,  3.9987e-05,  2.9140e-05,  ..., -3.2784e-06,\n",
      "           -1.8545e-05,  1.2086e-05],\n",
      "          [-6.6297e-06,  2.4681e-05,  2.2358e-05,  ...,  2.2353e-05,\n",
      "           -8.6557e-06, -3.0921e-05],\n",
      "          ...,\n",
      "          [ 5.2419e-07,  9.2853e-06,  1.3302e-05,  ...,  2.2347e-05,\n",
      "           -1.9045e-05, -1.3420e-06],\n",
      "          [-2.0405e-05, -4.9075e-05,  5.7452e-05,  ...,  9.8863e-07,\n",
      "            3.4020e-05,  1.8492e-05],\n",
      "          [-6.5501e-06, -2.7565e-05,  2.7096e-05,  ..., -8.3002e-06,\n",
      "            1.5855e-05,  3.0314e-05]]]])\n",
      "torch.Size([64, 300])\n",
      "Batch\n",
      "tensor([[[ 0.0261],\n",
      "         [ 0.0979],\n",
      "         [-0.0401],\n",
      "         ...,\n",
      "         [-0.0251],\n",
      "         [-0.0500],\n",
      "         [-0.0133]],\n",
      "\n",
      "        [[ 0.0261],\n",
      "         [ 0.0979],\n",
      "         [-0.0401],\n",
      "         ...,\n",
      "         [-0.0251],\n",
      "         [-0.0500],\n",
      "         [-0.0133]],\n",
      "\n",
      "        [[ 0.0009],\n",
      "         [-0.0564],\n",
      "         [ 0.0160],\n",
      "         ...,\n",
      "         [ 0.0836],\n",
      "         [-0.0579],\n",
      "         [-0.0212]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0273],\n",
      "         [-0.0372],\n",
      "         [-0.0413],\n",
      "         ...,\n",
      "         [-0.0141],\n",
      "         [ 0.0174],\n",
      "         [-0.0078]],\n",
      "\n",
      "        [[ 0.0261],\n",
      "         [ 0.0979],\n",
      "         [-0.0401],\n",
      "         ...,\n",
      "         [-0.0251],\n",
      "         [-0.0500],\n",
      "         [-0.0133]],\n",
      "\n",
      "        [[-0.0273],\n",
      "         [-0.0372],\n",
      "         [-0.0413],\n",
      "         ...,\n",
      "         [-0.0141],\n",
      "         [ 0.0174],\n",
      "         [-0.0078]]])\n",
      "Matrices\n",
      "Parameter containing:\n",
      "tensor([[[[ 1.0172e+00, -1.3332e-02,  1.5858e-03,  ..., -4.4518e-03,\n",
      "            2.4159e-02,  1.1907e-02],\n",
      "          [ 2.1533e-03,  9.9865e-01,  2.1283e-02,  ...,  9.1065e-03,\n",
      "           -4.5892e-03,  1.4043e-02],\n",
      "          [-1.6871e-02,  8.9714e-03,  1.0000e+00,  ...,  7.7264e-04,\n",
      "           -1.3909e-02, -2.4764e-02],\n",
      "          ...,\n",
      "          [-1.6390e-02, -2.5964e-02,  2.0136e-02,  ...,  1.0251e+00,\n",
      "            3.2016e-04,  7.4776e-03],\n",
      "          [ 1.6501e-02, -1.1107e-02, -3.7671e-02,  ...,  3.3319e-02,\n",
      "            9.7075e-01, -2.5178e-02],\n",
      "          [-1.2188e-02, -1.2239e-02,  2.9540e-03,  ...,  6.0745e-03,\n",
      "            1.2139e-03,  9.9917e-01]]]], requires_grad=True)\n",
      "Projections\n",
      "tensor([[[ 0.0133, -0.0039,  0.0470,  ..., -0.0057,  0.1015, -0.0431]],\n",
      "\n",
      "        [[ 0.0133, -0.0039,  0.0470,  ..., -0.0057,  0.1015, -0.0431]],\n",
      "\n",
      "        [[-0.0709, -0.1480,  0.1628,  ...,  0.1747,  0.0724, -0.0510]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0570, -0.1829,  0.1402,  ...,  0.0776,  0.1962,  0.0059]],\n",
      "\n",
      "        [[ 0.0133, -0.0039,  0.0470,  ..., -0.0057,  0.1015, -0.0431]],\n",
      "\n",
      "        [[-0.0570, -0.1829,  0.1402,  ...,  0.0776,  0.1962,  0.0059]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Similarities\n",
      "tensor([[-0.5005],\n",
      "        [-0.3749],\n",
      "        [-0.2985],\n",
      "        [-0.4330],\n",
      "        [-0.4086],\n",
      "        [-0.1982],\n",
      "        [-0.5780],\n",
      "        [-0.3058],\n",
      "        [-0.4375],\n",
      "        [-0.1706],\n",
      "        [-0.3291],\n",
      "        [-0.3251],\n",
      "        [-0.4641],\n",
      "        [-0.4459],\n",
      "        [-0.4376],\n",
      "        [-0.4092],\n",
      "        [-0.1851],\n",
      "        [-0.3979],\n",
      "        [-0.2396],\n",
      "        [-0.3911],\n",
      "        [-0.3687],\n",
      "        [-0.4422],\n",
      "        [-0.4153],\n",
      "        [-0.1003],\n",
      "        [-0.5010],\n",
      "        [-0.4792],\n",
      "        [-0.4793],\n",
      "        [-0.2340],\n",
      "        [-0.5068],\n",
      "        [-0.5059],\n",
      "        [-0.4288],\n",
      "        [-0.3241],\n",
      "        [-0.3860],\n",
      "        [-0.2521],\n",
      "        [-0.3480],\n",
      "        [-0.4362],\n",
      "        [-0.3026],\n",
      "        [-0.4879],\n",
      "        [-0.2953],\n",
      "        [-0.3746],\n",
      "        [-0.2648],\n",
      "        [-0.1147],\n",
      "        [-0.3973],\n",
      "        [-0.4023],\n",
      "        [-0.3766],\n",
      "        [-0.2515],\n",
      "        [-0.5350],\n",
      "        [-0.2824],\n",
      "        [-0.4855],\n",
      "        [-0.3044],\n",
      "        [-0.3409],\n",
      "        [-0.3356],\n",
      "        [-0.3554],\n",
      "        [-0.4905],\n",
      "        [-0.4287],\n",
      "        [-0.5238],\n",
      "        [-0.4504],\n",
      "        [-0.1665],\n",
      "        [-0.4802],\n",
      "        [-0.3087],\n",
      "        [-0.3810],\n",
      "        [-0.3571],\n",
      "        [-0.2209],\n",
      "        [-0.4325]], grad_fn=<DivBackward0>)\n",
      "Logits\n",
      "tensor([[0.6142],\n",
      "        [0.6347],\n",
      "        [0.6471],\n",
      "        [0.6252],\n",
      "        [0.6292],\n",
      "        [0.6635],\n",
      "        [0.6016],\n",
      "        [0.6460],\n",
      "        [0.6245],\n",
      "        [0.6680],\n",
      "        [0.6422],\n",
      "        [0.6428],\n",
      "        [0.6202],\n",
      "        [0.6231],\n",
      "        [0.6245],\n",
      "        [0.6291],\n",
      "        [0.6656],\n",
      "        [0.6309],\n",
      "        [0.6567],\n",
      "        [0.6320],\n",
      "        [0.6357],\n",
      "        [0.6237],\n",
      "        [0.6281],\n",
      "        [0.6794],\n",
      "        [0.6141],\n",
      "        [0.6177],\n",
      "        [0.6177],\n",
      "        [0.6576],\n",
      "        [0.6132],\n",
      "        [0.6133],\n",
      "        [0.6259],\n",
      "        [0.6430],\n",
      "        [0.6329],\n",
      "        [0.6547],\n",
      "        [0.6391],\n",
      "        [0.6247],\n",
      "        [0.6465],\n",
      "        [0.6163],\n",
      "        [0.6477],\n",
      "        [0.6347],\n",
      "        [0.6526],\n",
      "        [0.6771],\n",
      "        [0.6310],\n",
      "        [0.6302],\n",
      "        [0.6344],\n",
      "        [0.6548],\n",
      "        [0.6086],\n",
      "        [0.6498],\n",
      "        [0.6167],\n",
      "        [0.6462],\n",
      "        [0.6402],\n",
      "        [0.6411],\n",
      "        [0.6379],\n",
      "        [0.6159],\n",
      "        [0.6259],\n",
      "        [0.6104],\n",
      "        [0.6224],\n",
      "        [0.6686],\n",
      "        [0.6175],\n",
      "        [0.6455],\n",
      "        [0.6337],\n",
      "        [0.6376],\n",
      "        [0.6598],\n",
      "        [0.6253]], grad_fn=<AddmmBackward>)\n",
      "Matrix grad\n",
      "tensor([[[[ 7.0869e-06,  3.8702e-05,  1.1877e-06,  ..., -8.7231e-06,\n",
      "           -9.0154e-06,  6.1570e-06],\n",
      "          [ 1.1510e-05,  2.0724e-05, -4.6311e-06,  ..., -1.1321e-05,\n",
      "            4.9136e-07,  8.3367e-06],\n",
      "          [-1.3983e-05, -1.9537e-05,  3.6312e-05,  ...,  2.8707e-05,\n",
      "           -7.0518e-06, -5.5353e-06],\n",
      "          ...,\n",
      "          [-1.4709e-05, -5.1454e-05,  1.0418e-05,  ...,  3.5239e-05,\n",
      "            4.5017e-06, -1.2606e-05],\n",
      "          [-4.1191e-06, -4.0530e-06, -2.5509e-05,  ..., -2.1763e-05,\n",
      "            2.4007e-05,  4.0267e-06],\n",
      "          [-6.5278e-06, -2.3164e-05,  3.0620e-05,  ...,  2.3347e-05,\n",
      "           -2.4854e-06,  4.8082e-06]]]])\n",
      "torch.Size([56, 300])\n",
      "Batch\n",
      "tensor([[[ 0.0229],\n",
      "         [-0.0117],\n",
      "         [-0.0763],\n",
      "         ...,\n",
      "         [ 0.0146],\n",
      "         [ 0.0227],\n",
      "         [-0.0775]],\n",
      "\n",
      "        [[-0.0273],\n",
      "         [-0.0372],\n",
      "         [-0.0413],\n",
      "         ...,\n",
      "         [-0.0141],\n",
      "         [ 0.0174],\n",
      "         [-0.0078]],\n",
      "\n",
      "        [[ 0.0261],\n",
      "         [ 0.0979],\n",
      "         [-0.0401],\n",
      "         ...,\n",
      "         [-0.0251],\n",
      "         [-0.0500],\n",
      "         [-0.0133]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0273],\n",
      "         [-0.0372],\n",
      "         [-0.0413],\n",
      "         ...,\n",
      "         [-0.0141],\n",
      "         [ 0.0174],\n",
      "         [-0.0078]],\n",
      "\n",
      "        [[-0.0273],\n",
      "         [-0.0372],\n",
      "         [-0.0413],\n",
      "         ...,\n",
      "         [-0.0141],\n",
      "         [ 0.0174],\n",
      "         [-0.0078]],\n",
      "\n",
      "        [[ 0.0009],\n",
      "         [-0.0564],\n",
      "         [ 0.0160],\n",
      "         ...,\n",
      "         [ 0.0836],\n",
      "         [-0.0579],\n",
      "         [-0.0212]]])\n",
      "Matrices\n",
      "Parameter containing:\n",
      "tensor([[[[ 1.0157e+00, -1.3443e-02,  1.0413e-03,  ..., -5.4650e-03,\n",
      "            2.4595e-02,  1.2417e-02],\n",
      "          [ 1.4071e-03,  9.9761e-01,  2.2434e-02,  ...,  9.2242e-03,\n",
      "           -3.7195e-03,  1.5143e-02],\n",
      "          [-1.7279e-02,  7.7830e-03,  9.9815e-01,  ...,  1.2948e-03,\n",
      "           -1.4264e-02, -2.5811e-02],\n",
      "          ...,\n",
      "          [-1.6662e-02, -2.6089e-02,  2.0024e-02,  ...,  1.0246e+00,\n",
      "           -4.6975e-05,  9.3038e-03],\n",
      "          [ 1.8153e-02, -1.0032e-02, -3.9452e-02,  ...,  3.4756e-02,\n",
      "            9.6869e-01, -2.7342e-02],\n",
      "          [-1.1128e-02, -1.0805e-02,  1.3176e-03,  ...,  4.9293e-03,\n",
      "            1.3018e-03,  9.9726e-01]]]], requires_grad=True)\n",
      "Projections\n",
      "tensor([[[-0.0835, -0.1264,  0.0609,  ..., -0.0257,  0.1961, -0.0364]],\n",
      "\n",
      "        [[-0.0466, -0.1870,  0.1536,  ...,  0.0813,  0.2099,  0.0147]],\n",
      "\n",
      "        [[ 0.0185, -0.0138,  0.0519,  ..., -0.0024,  0.1175, -0.0344]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0466, -0.1870,  0.1536,  ...,  0.0813,  0.2099,  0.0147]],\n",
      "\n",
      "        [[-0.0466, -0.1870,  0.1536,  ...,  0.0813,  0.2099,  0.0147]],\n",
      "\n",
      "        [[-0.0709, -0.1516,  0.1707,  ...,  0.1743,  0.0853, -0.0490]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Similarities\n",
      "tensor([[-0.3446],\n",
      "        [-0.4185],\n",
      "        [-0.3930],\n",
      "        [-0.3574],\n",
      "        [-0.4322],\n",
      "        [-0.4548],\n",
      "        [-0.3926],\n",
      "        [-0.3313],\n",
      "        [-0.0925],\n",
      "        [-0.3444],\n",
      "        [-0.1092],\n",
      "        [-0.3333],\n",
      "        [-0.5499],\n",
      "        [-0.3560],\n",
      "        [-0.1453],\n",
      "        [-0.3257],\n",
      "        [-0.4696],\n",
      "        [-0.3958],\n",
      "        [-0.4739],\n",
      "        [-0.3937],\n",
      "        [-0.1675],\n",
      "        [-0.3273],\n",
      "        [-0.3855],\n",
      "        [-0.3724],\n",
      "        [-0.4848],\n",
      "        [-0.5137],\n",
      "        [-0.4028],\n",
      "        [-0.4835],\n",
      "        [-0.2685],\n",
      "        [-0.4311],\n",
      "        [-0.4234],\n",
      "        [-0.5846],\n",
      "        [-0.5070],\n",
      "        [-0.3830],\n",
      "        [-0.1926],\n",
      "        [-0.4854],\n",
      "        [-0.4089],\n",
      "        [-0.3474],\n",
      "        [-0.4754],\n",
      "        [-0.1086],\n",
      "        [-0.3137],\n",
      "        [-0.4262],\n",
      "        [-0.5184],\n",
      "        [-0.2704],\n",
      "        [-0.5076],\n",
      "        [-0.5443],\n",
      "        [-0.3827],\n",
      "        [-0.3590],\n",
      "        [-0.4225],\n",
      "        [-0.4630],\n",
      "        [-0.3335],\n",
      "        [-0.1671],\n",
      "        [-0.4214],\n",
      "        [-0.4615],\n",
      "        [-0.2792],\n",
      "        [-0.4643]], grad_fn=<DivBackward0>)\n",
      "Logits\n",
      "tensor([[0.6357],\n",
      "        [0.6235],\n",
      "        [0.6277],\n",
      "        [0.6336],\n",
      "        [0.6212],\n",
      "        [0.6175],\n",
      "        [0.6278],\n",
      "        [0.6379],\n",
      "        [0.6775],\n",
      "        [0.6358],\n",
      "        [0.6747],\n",
      "        [0.6376],\n",
      "        [0.6018],\n",
      "        [0.6339],\n",
      "        [0.6688],\n",
      "        [0.6389],\n",
      "        [0.6151],\n",
      "        [0.6273],\n",
      "        [0.6143],\n",
      "        [0.6276],\n",
      "        [0.6651],\n",
      "        [0.6386],\n",
      "        [0.6290],\n",
      "        [0.6311],\n",
      "        [0.6125],\n",
      "        [0.6078],\n",
      "        [0.6261],\n",
      "        [0.6127],\n",
      "        [0.6483],\n",
      "        [0.6214],\n",
      "        [0.6227],\n",
      "        [0.5960],\n",
      "        [0.6089],\n",
      "        [0.6294],\n",
      "        [0.6609],\n",
      "        [0.6124],\n",
      "        [0.6251],\n",
      "        [0.6353],\n",
      "        [0.6141],\n",
      "        [0.6748],\n",
      "        [0.6409],\n",
      "        [0.6222],\n",
      "        [0.6070],\n",
      "        [0.6480],\n",
      "        [0.6088],\n",
      "        [0.6027],\n",
      "        [0.6294],\n",
      "        [0.6334],\n",
      "        [0.6228],\n",
      "        [0.6161],\n",
      "        [0.6376],\n",
      "        [0.6651],\n",
      "        [0.6230],\n",
      "        [0.6164],\n",
      "        [0.6466],\n",
      "        [0.6159]], grad_fn=<AddmmBackward>)\n",
      "Matrix grad\n",
      "tensor([[[[ 4.1474e-06,  2.1725e-05,  9.1538e-06,  ..., -5.1240e-07,\n",
      "           -1.6959e-05,  5.1046e-06],\n",
      "          [ 1.7395e-05,  5.3977e-05, -1.1713e-05,  ..., -1.1081e-05,\n",
      "           -3.5238e-05, -2.6370e-06],\n",
      "          [ 1.1430e-06,  1.4413e-06, -6.5350e-06,  ..., -2.9602e-06,\n",
      "           -2.1213e-06,  2.9002e-06],\n",
      "          ...,\n",
      "          [ 1.6679e-05,  2.2591e-05, -1.4734e-06,  ...,  8.1470e-07,\n",
      "           -1.7927e-06, -6.1908e-06],\n",
      "          [ 2.1830e-06,  2.5263e-05, -4.4822e-06,  ..., -3.6456e-05,\n",
      "            2.5651e-05,  1.6359e-05],\n",
      "          [ 8.4563e-06,  7.7248e-06, -2.7457e-06,  ...,  2.1121e-06,\n",
      "           -3.5148e-06,  2.3064e-06]]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print('Epoch {}. Started training.'.format(epoch + 1))\n",
    "\n",
    "    for it, batch in tqdm(enumerate(data_loader), total=len(dataset) / batch_size):\n",
    "        model.train()\n",
    "        hyponyms, candidates, labels = batch\n",
    "        \n",
    "        print(hyponyms.shape)\n",
    "        \n",
    "        model_batch = {\n",
    "            'batch': hyponyms,\n",
    "            'candidate': candidates\n",
    "        }\n",
    "\n",
    "        labels = labels.float()\n",
    "        logits = model(model_batch).squeeze()\n",
    "        loss = loss_fn(logits, labels)\n",
    "        \n",
    "        if np.isnan(loss.detach().numpy()):\n",
    "            print(probas)\n",
    "            break\n",
    "        \n",
    "        if len(loss_history) % val_freq == 0:\n",
    "            test_loss_history.append(eval_loss_on_test(model))\n",
    "        loss_history.append(loss.detach().numpy())\n",
    "        matrix_norm = np.linalg.norm(model.matrices.detach().numpy())\n",
    "        matrix_norms.append(matrix_norm)\n",
    "        weight_norm = np.linalg.norm(model.prob_layer.weight.detach().numpy())\n",
    "        weight_norms.append(weight_norm)\n",
    "        \n",
    "        if len(loss_history) % plot_frequency == 0:\n",
    "            print(loss)\n",
    "            clear_output(wait=True)\n",
    "            plt.figure(figsize=(20, 7))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.title('Loss history', fontsize=18)\n",
    "            plt.xlabel('Iteration', fontsize=15)\n",
    "            plt.ylabel('CE loss', fontsize=15)\n",
    "            plt.plot(np.array(loss_history), label='train')\n",
    "            plt.plot(np.arange(len(test_loss_history)) * val_freq, np.array(test_loss_history), label='test')\n",
    "            plt.legend(fontsize=15)\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.title('Matrix norm history', fontsize=18)\n",
    "            plt.xlabel('Iteration', fontsize=15)\n",
    "            plt.ylabel('L2 Norm', fontsize=15)\n",
    "            plt.plot(np.array(matrix_norms))\n",
    "            plt.plot(np.array(weight_norms))\n",
    "            \n",
    "            plt.show()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        \n",
    "        print('Matrix grad')\n",
    "        print(model.matrices.grad)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../data/models/projection_model_single.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.999999523162842, 0.999998688697815, 0.999999761581421,\n",
       "        0.999999046325684, 0.999999880790710, 0.999999523162842,\n",
       "        0.999999284744263, 0.999999642372131, 0.999999403953552,\n",
       "        0.999999761581421, 0.999999165534973, 0.999998927116394,\n",
       "        1.000000000000000, 0.999999165534973, 0.999997735023499,\n",
       "        0.999998688697815, 0.999999284744263, 0.999998927116394,\n",
       "        0.999998569488525, 0.999999284744263, 0.999999284744263,\n",
       "        0.999999642372131, 0.999999523162842, 0.999998688697815,\n",
       "        0.999999165534973, 0.999998450279236, 0.999999523162842,\n",
       "        0.999999761581421, 0.999999523162842, 0.999999403953552,\n",
       "        0.999999642372131, 0.999999284744263, 0.999998569488525,\n",
       "        0.999999523162842, 0.999999761581421, 0.999999046325684,\n",
       "        0.999999284744263, 0.999999761581421, 0.999999523162842,\n",
       "        0.999999523162842, 0.999999523162842, 0.999999761581421,\n",
       "        0.999999642372131, 0.999999761581421, 0.999997854232788,\n",
       "        0.999999642372131, 0.999999284744263, 0.999999642372131,\n",
       "        0.999998331069946, 0.999999642372131, 0.999999403953552,\n",
       "        0.999999403953552, 0.999998450279236, 0.999999761581421,\n",
       "        0.999999642372131, 0.999999642372131, 0.999999761581421,\n",
       "        0.999998569488525, 0.999997973442078, 0.999999165534973,\n",
       "        0.999999761581421, 0.999998450279236, 0.999998927116394,\n",
       "        0.999998688697815], grad_fn=<ClampBackward>)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa179321470>]"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 7))\n",
    "            \n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Loss history', fontsize=18)\n",
    "plt.xlabel('Iteration', fontsize=15)\n",
    "plt.ylabel('CE loss', fontsize=15)\n",
    "plt.plot(np.array(loss_history), label='train')\n",
    "plt.plot(np.arange(len(test_loss_history)) * val_freq, np.array(test_loss_history), label='test')\n",
    "plt.legend(fontsize=15)\n",
    "           \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Matrix norm history', fontsize=18)\n",
    "plt.xlabel('Iteration', fontsize=15)\n",
    "plt.ylabel('L2 Norm', fontsize=15)\n",
    "plt.plot(np.array(matrix_norms))\n",
    "plt.plot(np.array(weight_norms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.003188,\n",
       " 10.006206,\n",
       " 10.009219,\n",
       " 10.012233,\n",
       " 10.015238,\n",
       " 10.01826,\n",
       " 10.021319,\n",
       " 10.0243845,\n",
       " 10.027425,\n",
       " 10.030435]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_norms[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          ...,\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[nan]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([nan], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(-11.564904, dtype=float32),\n",
       " array(-10.748442, dtype=float32),\n",
       " array(-10.634823, dtype=float32),\n",
       " array(-10.231451, dtype=float32),\n",
       " array(-11.301371, dtype=float32),\n",
       " array(-11.715054, dtype=float32),\n",
       " array(-10.933878, dtype=float32),\n",
       " array(-10.134322, dtype=float32),\n",
       " array(-9.730699, dtype=float32),\n",
       " array(-inf, dtype=float32)]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_history[-10:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
